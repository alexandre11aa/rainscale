{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 08:06:47.386070: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-14 08:06:47.391638: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-14 08:06:47.406643: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747220807.423794 3134867 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747220807.429875 3134867 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747220807.448734 3134867 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747220807.448751 3134867 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747220807.448753 3134867 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747220807.448754 3134867 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-14 08:06:47.452871: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolacao_por_ml(df: pd.DataFrame,\n",
    "                        col_de_treino: list[str],\n",
    "                        var_de_predicao: str,\n",
    "                        var_de_pontos: str,\n",
    "                        n_de_teste: int):\n",
    "\n",
    "    # Obtendo pontos únicos\n",
    "    pontos_unicos = df[var_de_pontos].unique()\n",
    "\n",
    "    # Definição dos modelos\n",
    "    models = [\n",
    "\n",
    "        (\"ExtraTrees\", ExtraTreesRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=20,\n",
    "            max_features=2,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            random_state=7\n",
    "        ))\n",
    "\n",
    "    ]\n",
    "\n",
    "    # Defininção de lista de métricas\n",
    "    metrics = []\n",
    "    for i in range(len(models)):\n",
    "        metrics.append([[], []])\n",
    "\n",
    "    for i in range(n_de_teste):\n",
    "\n",
    "        # Embaralha os pontos únicos\n",
    "        np.random.shuffle(pontos_unicos)\n",
    "\n",
    "        # Dividindo em 70% treino e 30% teste\n",
    "        split_idx = int(len(pontos_unicos) * 0.8)\n",
    "        pontos_treino = set(pontos_unicos[:split_idx])\n",
    "        pontos_teste = set(pontos_unicos[split_idx:])\n",
    "\n",
    "        # Criando DataFrames de treino e teste\n",
    "        df_treino = df[df[var_de_pontos].isin(pontos_treino)].copy()\n",
    "        df_teste = df[df[var_de_pontos].isin(pontos_teste)].copy()\n",
    "\n",
    "        # Definindo features (X) e variável alvo (y)\n",
    "        X_train = df_treino[col_de_treino]\n",
    "        y_train = df_treino[var_de_predicao]\n",
    "\n",
    "        X_test = df_teste[col_de_treino]\n",
    "        y_test = df_teste[var_de_predicao]\n",
    "\n",
    "        # Treinar e avaliar cada modelo\n",
    "        for j in range(len(models)):\n",
    "            models[j][1].fit(X_train, y_train)                                 # Treinamento\n",
    "            y_pred = models[j][1].predict(X_test)                              # Previsão\n",
    "            metrics[j][0].append(r2_score(y_test, y_pred))                     # R²\n",
    "            metrics[j][1].append(np.sqrt(mean_squared_error(y_test, y_pred)))  # RMSE\n",
    "\n",
    "    # Verificando melhor Modelo a partir de r2\n",
    "    best_model = (0, '', '')\n",
    "\n",
    "    print('Verificação de Modelos:\\n')\n",
    "\n",
    "    for i in range(len(metrics)):\n",
    "\n",
    "        r2, rmse = np.mean(metrics[i][0]), np.mean(metrics[i][1])\n",
    "\n",
    "        if best_model[0] < r2:\n",
    "            best_model = r2, models[i][0], models[i][1]\n",
    "\n",
    "        print(f'Modelo: {models[i][0][:3]} \\t R²: {r2:.4f} \\t RMSE: {rmse:.4f}')\n",
    "\n",
    "    print(f'\\nO melhor modelo de ML para a base de dados é: {best_model[1]}.')\n",
    "\n",
    "    return best_model[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificação de Modelos:\n",
      "\n",
      "Modelo: Ext \t R²: 0.7420 \t RMSE: 41.2040\n",
      "\n",
      "O melhor modelo de ML para a base de dados é: ExtraTrees.\n"
     ]
    }
   ],
   "source": [
    "df_aesa = pd.read_csv(f'../datas/interim/3.2.1_aesa_with_idw/aesa_1994-2023_mon_sum_idw.csv')\n",
    "\n",
    "columns_X = [\"lat\", \"lon\", \"ano\", \"mes\", \"IDW\"]\n",
    "\n",
    "# Escolhendo melhor modelo preditivo\n",
    "model = interpolacao_por_ml(df_aesa, columns_X, \"pr_local\", \"pnt\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolacao_por_cnn(df: pd.DataFrame,\n",
    "                         col_de_treino: list[str],\n",
    "                         var_de_predicao: str,\n",
    "                         var_de_pontos: str,\n",
    "                         n_de_teste: int):\n",
    "\n",
    "    # Pontos únicos\n",
    "    pontos_unicos = df[var_de_pontos].unique()\n",
    "\n",
    "    # Listas para métricas\n",
    "    r2_list = []\n",
    "    rmse_list = []\n",
    "\n",
    "    for i in range(n_de_teste):\n",
    "\n",
    "        np.random.shuffle(pontos_unicos)\n",
    "        split_idx = int(len(pontos_unicos) * 0.8)\n",
    "\n",
    "        pontos_treino = set(pontos_unicos[:split_idx])\n",
    "        pontos_teste = set(pontos_unicos[split_idx:])\n",
    "\n",
    "        df_treino = df[df[var_de_pontos].isin(pontos_treino)].copy()\n",
    "        df_teste = df[df[var_de_pontos].isin(pontos_teste)].copy()\n",
    "\n",
    "        # Definindo X e y\n",
    "        X_train = df_treino[col_de_treino].values\n",
    "        y_train = df_treino[var_de_predicao].values\n",
    "\n",
    "        X_test = df_teste[col_de_treino].values\n",
    "        y_test = df_teste[var_de_predicao].values\n",
    "\n",
    "        # Redimensionar para 3D: (samples, timesteps=1, features)\n",
    "        X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "        X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "        # Limpar sessão anterior (importante em loops com Keras)\n",
    "        clear_session()\n",
    "\n",
    "        # Criando modelo CNN\n",
    "        cnn = Sequential([\n",
    "            Input(shape=(1, X_train.shape[2])),\n",
    "            Conv1D(64, kernel_size=3, padding='same', activation='relu'),\n",
    "            MaxPooling1D(1),\n",
    "            Flatten(),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "\n",
    "        cnn.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mse'])\n",
    "\n",
    "        # Treinamento\n",
    "        cnn.fit(X_train, y_train, epochs=10, validation_split=0.2, verbose=0)\n",
    "\n",
    "        # Previsão e avaliação\n",
    "        y_pred = cnn.predict(X_test).flatten()\n",
    "\n",
    "        r2_list.append(r2_score(y_test, y_pred))\n",
    "        rmse_list.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "    # Resultados finais\n",
    "    print(f\"\\nCNN \\t Média R²: {np.mean(r2_list):.4f} \\t Média RMSE: {np.mean(rmse_list):.4f}\")\n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 765us/step\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 880us/step\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 792us/step\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 976us/step\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 838us/step\n",
      "\n",
      "CNN \t Média R²: 0.7330 \t Média RMSE: 41.9505\n"
     ]
    }
   ],
   "source": [
    "df_aesa = pd.read_csv(f'../datas/interim/3.2.1_aesa_with_idw/aesa_1994-2023_mon_sum_idw.csv')\n",
    "\n",
    "columns_X = [\"lat\", \"lon\", \"ano\", \"mes\", \"IDW\"]\n",
    "\n",
    "# Escolhendo melhor modelo preditivo\n",
    "model = interpolacao_por_cnn(df_aesa, columns_X, \"pr_local\", \"pnt\", 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
