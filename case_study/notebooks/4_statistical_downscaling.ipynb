{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c897ee",
   "metadata": {},
   "source": [
    "# 4. Downscaling de Dados Locais e Dados do CMIP6\n",
    "\n",
    "```python\n",
    "Esse caderno tem como objetivo a obtenção da precipitação futura de dados locais \n",
    "para os pontos definidos nos GCMs do CMIP6 a partir de predição.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1867920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from geopy.distance import geodesic\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    ExtraTreesRegressor,\n",
    "    GradientBoostingRegressor\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv1D,\n",
    "    MaxPooling1D,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50625fc1",
   "metadata": {},
   "source": [
    "## 4.1. Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82b688c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição de se vai ocorrer ou não a geração de bases de dados\n",
    "databases_generate = False\n",
    "\n",
    "# Definição de se vai ocorrer ou não o a geração e o uso do método IDW\n",
    "idw_method = True\n",
    "idw_generate = False\n",
    "\n",
    "# Tipo de base de dados local utilizada ('sum' ou 'max')\n",
    "database_type = 'sum'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9ce24e",
   "metadata": {},
   "source": [
    "## 4.2. Funções"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae316f3",
   "metadata": {},
   "source": [
    "### 4.2.1. Função para Limeza de Terminal e Células"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49f72a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear():\n",
    "    '''\n",
    "    Função para limpar terminal ou célula\n",
    "    '''\n",
    "\n",
    "    # Limpando terminal\n",
    "    # os.system('cls')\n",
    "\n",
    "    # Limpando célula\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9052cd18",
   "metadata": {},
   "source": [
    "### 4.2.2. Função para ficar Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b34d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_seed(seed):\n",
    "    '''\n",
    "    Fixa seed informada\n",
    "    '''\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd782b8c",
   "metadata": {},
   "source": [
    "### 4.2.3. Predição a partir de Modelos de Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36ed126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicao_por_ml(df: pd.DataFrame,\n",
    "                    col_de_treino: list[str],\n",
    "                    var_de_predicao: str,\n",
    "                    anos_X: tuple,\n",
    "                    anos_Y: tuple):\n",
    "\n",
    "    # Definição dos modelos\n",
    "    models = [\n",
    "\n",
    "        # (\"ExtraTrees\", ExtraTreesRegressor(\n",
    "        #     n_estimators=15,\n",
    "        #     max_depth=20,\n",
    "        #     max_features=2,\n",
    "        #     min_samples_split=2,\n",
    "        #     min_samples_leaf=1,\n",
    "        #     random_state=7\n",
    "        # )),\n",
    "\n",
    "        # (\"RandomForest\", RandomForestRegressor(\n",
    "        #     n_estimators=15,\n",
    "        #     max_depth=25,\n",
    "        #     max_features=2,\n",
    "        #     min_samples_split=2,\n",
    "        #     min_samples_leaf=1,\n",
    "        #     random_state=7\n",
    "        # )),\n",
    "\n",
    "        # (\"GradientBoosting\", GradientBoostingRegressor(\n",
    "        #     n_estimators=200,\n",
    "        #     learning_rate=0.05,\n",
    "        #     max_depth=5,\n",
    "        #     subsample=0.8,\n",
    "        #     random_state=7\n",
    "        # )),\n",
    "\n",
    "        (\"KNeighbors\", KNeighborsRegressor(\n",
    "            n_neighbors=7,\n",
    "            weights='distance',\n",
    "            algorithm='auto'\n",
    "        )),\n",
    "\n",
    "        (\"LinearRegression\", LinearRegression(\n",
    "            fit_intercept=True,\n",
    "            positive=False\n",
    "        ))\n",
    "\n",
    "    ]\n",
    "\n",
    "    best_model = ['', 0]\n",
    "\n",
    "    models_infos = []\n",
    "\n",
    "    for model in models:\n",
    "\n",
    "        # Lista de métricas por modelo\n",
    "        r_list, rmse_list = [], []\n",
    "\n",
    "        for i in range(len(anos_X)):\n",
    "\n",
    "            ano_X_inicial, ano_X_final = anos_X[i]\n",
    "            ano_y_inicial, ano_y_final = anos_Y[i]\n",
    "\n",
    "            df_treino = df[(df['ano'] >= ano_X_inicial) & (df['ano'] <= ano_X_final)].copy()\n",
    "            df_teste  = df[(df['ano'] >= ano_y_inicial) & (df['ano'] <= ano_y_final)].copy()\n",
    "\n",
    "            X_train = df_treino[col_de_treino]\n",
    "            y_train = df_treino[var_de_predicao]\n",
    "\n",
    "            X_test = df_teste[col_de_treino]\n",
    "            y_test = df_teste[var_de_predicao]\n",
    "\n",
    "            model[1].fit(X_train, y_train)\n",
    "            y_pred = model[1].predict(X_test)\n",
    "\n",
    "            r_list.append(np.corrcoef(y_test, y_pred)[0, 1])  # r\n",
    "            rmse_list.append(np.sqrt(mean_squared_error(y_test, y_pred)))  # RMSE\n",
    "\n",
    "        if np.mean(r_list) >= best_model[1]:\n",
    "            best_model[0] = model[1]\n",
    "            best_model[1] = np.mean(r_list)\n",
    "\n",
    "        # print(f\"{model[0][:3]} \\t Média r: {np.mean(r_list):.4f} \\t Média RMSE: {np.mean(rmse_list):.4f}\")\n",
    "\n",
    "        models_infos.append((model[0], np.mean(r_list), np.mean(rmse_list)))\n",
    "\n",
    "    # return (best_model[0], best_model[1])\n",
    "\n",
    "    return models_infos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac1ca00",
   "metadata": {},
   "source": [
    "### 4.2.4. Predição a partir de Modelos de Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicao_por_cnn(df: pd.DataFrame,\n",
    "                     col_de_treino: list[str],\n",
    "                     var_de_predicao: str,\n",
    "                     anos_X: tuple,\n",
    "                     anos_Y: tuple,\n",
    "                     seed=58):\n",
    "\n",
    "    # Fixando seed\n",
    "    fix_seed(seed)\n",
    "\n",
    "    r_list, rmse_list = [], []\n",
    "\n",
    "    for i in range(len(anos_X)):\n",
    "\n",
    "        ano_X_inicial, ano_X_final = anos_X[i]\n",
    "        ano_y_inicial, ano_y_final = anos_Y[i]\n",
    "\n",
    "        df_treino = df[(df['ano'] >= ano_X_inicial) & (df['ano'] <= ano_X_final)].copy()\n",
    "        df_teste  = df[(df['ano'] >= ano_y_inicial) & (df['ano'] <= ano_y_final)].copy()\n",
    "\n",
    "        X_train = df_treino[col_de_treino].values\n",
    "        y_train = df_treino[var_de_predicao].values\n",
    "\n",
    "        X_test = df_teste[col_de_treino].values\n",
    "        y_test = df_teste[var_de_predicao].values\n",
    "\n",
    "        # Normalização\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Redimensionar para 3D: (samples, timesteps=1, features)\n",
    "        X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "        X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "        # Limpar sessão anterior (importante em loops com Keras)Dropout\n",
    "        clear_session()\n",
    "\n",
    "        # Criando modelo CNN\n",
    "        cnn = Sequential([\n",
    "            Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            Conv1D(64, kernel_size=3, padding='same', activation='relu'),\n",
    "            MaxPooling1D(1),\n",
    "            Flatten(),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(68, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "\n",
    "        cnn.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mse'])\n",
    "\n",
    "        # Treinamento\n",
    "        cnn.fit(X_train, y_train, epochs=5, validation_split=0.2, verbose=0)\n",
    "\n",
    "        # Previsão e avaliação\n",
    "        y_pred = cnn.predict(X_test).flatten()\n",
    "\n",
    "        r_list.append(np.corrcoef(y_test, y_pred)[0, 1])\n",
    "        rmse_list.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "    clear()\n",
    "\n",
    "    # print(f\"\\nCNN \\t Média r: {np.mean(r_list):.4f} \\t Média RMSE: {np.mean(rmse_list):.4f}\")\n",
    "\n",
    "    return ('CNN', np.mean(r_list), np.mean(rmse_list))\n",
    "\n",
    "def predicao_por_mlp(df: pd.DataFrame,\n",
    "                     col_de_treino: list[str],\n",
    "                     var_de_predicao: str,\n",
    "                     anos_X: tuple,\n",
    "                     anos_Y: tuple,\n",
    "                     seed=58):\n",
    "\n",
    "    # Fixando seed\n",
    "    fix_seed(seed)\n",
    "\n",
    "    r_list, rmse_list = [], []\n",
    "\n",
    "    for i in range(len(anos_X)):\n",
    "\n",
    "        ano_X_inicial, ano_X_final = anos_X[i]\n",
    "        ano_y_inicial, ano_y_final = anos_Y[i]\n",
    "\n",
    "        df_treino = df[(df['ano'] >= ano_X_inicial) & (df['ano'] <= ano_X_final)].copy()\n",
    "        df_teste  = df[(df['ano'] >= ano_y_inicial) & (df['ano'] <= ano_y_final)].copy()\n",
    "\n",
    "        X_train = df_treino[col_de_treino].values\n",
    "        y_train = df_treino[var_de_predicao].values\n",
    "\n",
    "        X_test = df_teste[col_de_treino].values\n",
    "        y_test = df_teste[var_de_predicao].values\n",
    "\n",
    "        # Normalização\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Limpar sessão anterior (importante em loops com Keras)Dropout\n",
    "        clear_session()\n",
    "\n",
    "        # Criando modelo CNN\n",
    "        mlp = Sequential([\n",
    "            Input(shape=(X_train.shape[1],)),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "\n",
    "        mlp.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mse'])\n",
    "\n",
    "        # Treinamento\n",
    "        mlp.fit(X_train, y_train, epochs=5, validation_split=0.2, verbose=0)\n",
    "\n",
    "        # Previsão e avaliação\n",
    "        y_pred = mlp.predict(X_test).flatten()\n",
    "\n",
    "        r_list.append(np.corrcoef(y_test, y_pred)[0, 1])\n",
    "        rmse_list.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "    clear()\n",
    "\n",
    "    # print(f\"\\nMLP \\t Média r: {np.mean(r_list):.4f} \\t Média RMSE: {np.mean(rmse_list):.4f}\")\n",
    "\n",
    "    return ('MLP', np.mean(r_list), np.mean(rmse_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d055b3",
   "metadata": {},
   "source": [
    "### 4.2.5. Interpolação a partir de Modelos de Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7dae822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolacao_por_ml(df: pd.DataFrame,\n",
    "                        col_de_treino: list[str],\n",
    "                        var_de_predicao: str,\n",
    "                        var_de_pontos: str,\n",
    "                        n_de_teste: int):\n",
    "\n",
    "    # Obtendo pontos únicos\n",
    "    pontos_unicos = df[var_de_pontos].unique()\n",
    "\n",
    "    # Definição dos modelos\n",
    "    models = [\n",
    "\n",
    "        (\"ExtraTrees\", ExtraTreesRegressor(\n",
    "            n_estimators=15,\n",
    "            max_depth=20,\n",
    "            max_features=2,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            random_state=7\n",
    "        )),\n",
    "\n",
    "        (\"RandomForest\", RandomForestRegressor(\n",
    "            n_estimators=15,\n",
    "            max_depth=25,\n",
    "            max_features=2,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            random_state=7\n",
    "        )),\n",
    "\n",
    "        # (\"KNeighbors\", KNeighborsRegressor(\n",
    "        #     n_neighbors=7,\n",
    "        #     weights='distance',\n",
    "        #     algorithm='auto'\n",
    "        # )),\n",
    "\n",
    "        # (\"GradientBoosting\", GradientBoostingRegressor(\n",
    "        #     n_estimators=200,\n",
    "        #     learning_rate=0.05,\n",
    "        #     max_depth=5,\n",
    "        #     subsample=0.8,\n",
    "        #     random_state=7\n",
    "        # )),\n",
    "\n",
    "        (\"LinearRegression\", LinearRegression(\n",
    "            fit_intercept=True,\n",
    "            positive=False\n",
    "        ))\n",
    "\n",
    "    ]\n",
    "\n",
    "    # Defininção de lista de métricas\n",
    "    metrics = []\n",
    "    for i in range(len(models)):\n",
    "        metrics.append([[], []])\n",
    "\n",
    "    for i in range(n_de_teste):\n",
    "\n",
    "        # Embaralha os pontos únicos\n",
    "        np.random.shuffle(pontos_unicos)\n",
    "\n",
    "        # Dividindo em 70% treino e 30% teste\n",
    "        split_idx = int(len(pontos_unicos) * 0.8)\n",
    "        pontos_treino = set(pontos_unicos[:split_idx])\n",
    "        pontos_teste = set(pontos_unicos[split_idx:])\n",
    "\n",
    "        # Criando DataFrames de treino e teste\n",
    "        df_treino = df[df[var_de_pontos].isin(pontos_treino)].copy()\n",
    "        df_teste = df[df[var_de_pontos].isin(pontos_teste)].copy()\n",
    "\n",
    "        # Definindo features (X) e variável alvo (y)\n",
    "        X_train = df_treino[col_de_treino]\n",
    "        y_train = df_treino[var_de_predicao]\n",
    "\n",
    "        X_test = df_teste[col_de_treino]\n",
    "        y_test = df_teste[var_de_predicao]\n",
    "\n",
    "        # Treinar e avaliar cada modelo\n",
    "        for j in range(len(models)):\n",
    "            models[j][1].fit(X_train, y_train)                                 # Treinamento\n",
    "            y_pred = models[j][1].predict(X_test)                              # Previsão\n",
    "            metrics[j][0].append(np.corrcoef(y_test, y_pred)[0, 1])            # R²\n",
    "            metrics[j][1].append(np.sqrt(mean_squared_error(y_test, y_pred)))  # RMSE\n",
    "\n",
    "    # Verificando melhor Modelo a partir de r\n",
    "    best_model = (0, '', '')\n",
    "\n",
    "    print('Verificação de Modelos:\\n')\n",
    "\n",
    "    for i in range(len(metrics)):\n",
    "\n",
    "        r, rmse = np.mean(metrics[i][0]), np.mean(metrics[i][1])\n",
    "\n",
    "        if best_model[0] < r:\n",
    "            best_model = r, models[i][0], models[i][1]\n",
    "\n",
    "        print(f'Modelo: {models[i][0][:3]} \\t r: {r:.4f} \\t RMSE: {rmse:.4f}')\n",
    "\n",
    "    print(f'\\nO melhor modelo de ML para a base de dados é: {best_model[1]}.')\n",
    "\n",
    "    return best_model[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322fbd1b",
   "metadata": {},
   "source": [
    "### 4.2.6. Função que Adiciona coluna IDW à Base de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "243b880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def porcentagem_em_barra(valor_atual: int,\n",
    "                         valor_total: int) -> str:\n",
    "    \"\"\"\n",
    "    Gerador de barra de porcentagem a partir de valor atual e total.\n",
    "    \"\"\"\n",
    "\n",
    "    porcentagem = 100 * (valor_atual / valor_total)\n",
    "\n",
    "    completo   = '━' * (int(porcentagem))\n",
    "    incompleto = '╺' * (100 - int(porcentagem))\n",
    "\n",
    "    situacao = f'[{completo}{incompleto}] {porcentagem:.2f}% ({valor_atual} de {valor_total})'\n",
    "\n",
    "    return situacao\n",
    "\n",
    "def idw_interpolation(target_point, neighbors, values, const=2):\n",
    "    weights = []\n",
    "    for pt in neighbors:\n",
    "        dist = geodesic(target_point, pt).meters\n",
    "        if dist == 0:\n",
    "            continue  # Evita usar o próprio ponto\n",
    "        weights.append(1 / (dist ** const))\n",
    "    weights = np.array(weights)\n",
    "    values = np.array(values)\n",
    "    return np.sum(weights * values) / np.sum(weights) if np.sum(weights) > 0 else np.nan\n",
    "\n",
    "def calcular_precipitacao_idw(df: pd.DataFrame,\n",
    "                               var_de_interpolacao: str,\n",
    "                               var_de_latitude: str,\n",
    "                               var_de_longitude: str,\n",
    "                               var_de_anos: str,\n",
    "                               var_de_meses: str,\n",
    "                               n_vizinhos: int = 10,\n",
    "                               const: int = 2) -> pd.DataFrame:\n",
    "\n",
    "    df_resultado = df.copy()\n",
    "    df_resultado[\"IDW\"] = np.nan  # nova coluna para os resultados\n",
    "\n",
    "    total = len(df_resultado)\n",
    "    for i, idx in enumerate(df_resultado.index):\n",
    "\n",
    "        row = df_resultado.loc[idx]\n",
    "        ano, mes, lat, lon = row[var_de_anos], row[var_de_meses], row[var_de_latitude], row[var_de_longitude]\n",
    "        target_point = (lat, lon)\n",
    "\n",
    "        # Candidatos com mesma data, excluindo a própria linha\n",
    "        candidatos = df_resultado[(df_resultado[var_de_anos] == ano) &\n",
    "                                  (df_resultado[var_de_meses] == mes) &\n",
    "                                  (df_resultado.index != idx)].copy()\n",
    "\n",
    "        if candidatos.empty:\n",
    "            continue\n",
    "\n",
    "        # Calcular distâncias\n",
    "        candidatos['distancia'] = candidatos.apply(\n",
    "            lambda r: geodesic(target_point, (r[var_de_latitude], r[var_de_longitude])).meters, axis=1\n",
    "        )\n",
    "\n",
    "        # Selecionar os vizinhos mais próximos\n",
    "        vizinhos = candidatos.nsmallest(n_vizinhos, 'distancia')\n",
    "\n",
    "        if not vizinhos.empty:\n",
    "            viz_points = list(zip(vizinhos[var_de_latitude], vizinhos[var_de_longitude]))\n",
    "            viz_values = vizinhos[var_de_interpolacao].tolist()\n",
    "\n",
    "            interpolado = idw_interpolation(target_point, viz_points, viz_values, const)\n",
    "            df_resultado.at[idx, \"IDW\"] = interpolado\n",
    "\n",
    "        # Barra de progresso a cada 10 registros\n",
    "        if (i + 1) % 10 == 0 or (i + 1) == total:\n",
    "            clear()\n",
    "            print(porcentagem_em_barra(i + 1, total))\n",
    "\n",
    "    return df_resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1db9db",
   "metadata": {},
   "source": [
    "## 4.3. CNRM-CM6-1HR\n",
    "\n",
    "Redução de escala de dados locais e dados experimentais do CMIP6 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f7f7de",
   "metadata": {},
   "source": [
    "### 4.3.1. Coluna IDW para dados CNRM-CM6-1HR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a15e485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Informações do GCM:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64200 entries, 0 to 64199\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   lat     64200 non-null  float64\n",
      " 1   lon     64200 non-null  float64\n",
      " 2   ano     64200 non-null  int64  \n",
      " 3   mes     64200 non-null  int64  \n",
      " 4   pr      64200 non-null  float64\n",
      " 5   pnt     64200 non-null  object \n",
      " 6   IDW     64200 non-null  float64\n",
      "dtypes: float64(4), int64(2), object(1)\n",
      "memory usage: 3.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Definindo base de dados de GCM\n",
    "df_cnrm_cm6_1hr = pd.read_csv(\n",
    "    f\"../datas/interim/1.3.2_cmip6_database_create/pr_day_CNRM-CM6-1-HR_ssp585_r1i1p1f2_gr_19940101-21001231_{database_type}.csv\"\n",
    ")\n",
    "\n",
    "# Padronizando valores de longitude\n",
    "df_cnrm_cm6_1hr[\"lon\"] = df_cnrm_cm6_1hr[\"lon\"] - 360\n",
    "\n",
    "# Adicionando coluna de pontos\n",
    "df_cnrm_cm6_1hr['pnt'] = df_cnrm_cm6_1hr[\"lat\"].astype(str) + \";\" + df_cnrm_cm6_1hr[\"lon\"].astype(str)\n",
    "\n",
    "# Caso não tenha a coluna IDW nas \"colunas_de_interesse\", calcula-se o IDW\n",
    "if idw_generate == True:\n",
    "\n",
    "    df_cnrm_cm6_1hr = calcular_precipitacao_idw(df_cnrm_cm6_1hr, \"pr\", \"lat\", \"lon\", \"ano\", \"mes\")\n",
    "    df_cnrm_cm6_1hr.to_csv(f'../datas/interim/4.3.1_cmip6_with_idw/pr_day_CNRM-CM6-1-HR_ssp585_r1i1p1f2_gr_19940101-21001231_{database_type}_idw.csv')\n",
    "\n",
    "else:\n",
    "\n",
    "    df_cnrm_cm6_1hr = pd.read_csv(f'../datas/interim/4.3.1_cmip6_with_idw/pr_day_CNRM-CM6-1-HR_ssp585_r1i1p1f2_gr_19940101-21001231_{database_type}_idw.csv')\n",
    "\n",
    "# Definindo colunas de interesse\n",
    "colunas_gcm = ['lat', 'lon', 'ano', 'mes', 'pr', 'pnt', 'IDW']\n",
    "\n",
    "# Restringindo base de dados às colunas de interesse\n",
    "df_cnrm_cm6_1hr = df_cnrm_cm6_1hr[colunas_gcm].copy()\n",
    "\n",
    "# Visualizando Bases de Dados do GCM\n",
    "print('- Informações do GCM:')\n",
    "print(df_cnrm_cm6_1hr.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ee1664",
   "metadata": {},
   "source": [
    "### 4.3.2. Criação de Bases de Dados de Redução de Escala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4186b7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18000 entries, 0 to 17999\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   lat       18000 non-null  float64\n",
      " 1   lon       18000 non-null  float64\n",
      " 2   ano       18000 non-null  int64  \n",
      " 3   mes       18000 non-null  int64  \n",
      " 4   pr_local  18000 non-null  float64\n",
      " 5   pnt       18000 non-null  object \n",
      "dtypes: float64(3), int64(2), object(1)\n",
      "memory usage: 843.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Importando base de dados\n",
    "df_aesa_to_cnrm_cm6_1hr = pd.read_csv(f'../datas/interim/3.3.3_aesa_interpolated_to_cmip6/aesa_to_cnrm_cm6_1hr_{database_type}_interpolated.csv')\n",
    "\n",
    "# Definindo colunas de interesse\n",
    "colunas_local = ['lat', 'lon', 'ano', 'mes', 'pr_local', 'pnt']\n",
    "\n",
    "# Redefinindo logitude\n",
    "df_aesa_to_cnrm_cm6_1hr['lon'] = df_aesa_to_cnrm_cm6_1hr['lon'] - 360\n",
    "\n",
    "# Adicionando coluna de pontos\n",
    "df_aesa_to_cnrm_cm6_1hr['pnt'] = df_aesa_to_cnrm_cm6_1hr[\"lat\"].astype(str) + \";\" + df_aesa_to_cnrm_cm6_1hr[\"lon\"].astype(str)\n",
    "\n",
    "# Restringindo base de dados às colunas de interesse\n",
    "df_aesa_to_cnrm_cm6_1hr = df_aesa_to_cnrm_cm6_1hr[colunas_local].copy()\n",
    "\n",
    "# Observando informações da base de dados\n",
    "df_aesa_to_cnrm_cm6_1hr.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5971b401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64200 entries, 0 to 64199\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   lat       64200 non-null  float64\n",
      " 1   lon       64200 non-null  float64\n",
      " 2   ano       64200 non-null  int64  \n",
      " 3   mes       64200 non-null  int64  \n",
      " 4   pr_local  18000 non-null  float64\n",
      " 5   pnt       64200 non-null  object \n",
      " 6   pr        64200 non-null  float64\n",
      " 7   IDW       64200 non-null  float64\n",
      "dtypes: float64(5), int64(2), object(1)\n",
      "memory usage: 3.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Fazendo o merge com base em 'pnt', 'ano' e 'mes' para base de dados única\n",
    "df_aesa_to_cnrm_cm6_1hr = df_aesa_to_cnrm_cm6_1hr.merge(df_cnrm_cm6_1hr, on=['pnt', 'ano', 'mes', 'lat', 'lon'], how='outer')\n",
    "\n",
    "# Salvando nova base de dados\n",
    "if databases_generate:\n",
    "    df_aesa_to_cnrm_cm6_1hr.to_csv(f'../datas/interim/4.3.2_create_downscaling_database/aesa_to_cnrm_cm6_1hr_{database_type}_downscaling.csv')\n",
    "\n",
    "# Informações da base de dados única\n",
    "df_aesa_to_cnrm_cm6_1hr.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502ad727",
   "metadata": {},
   "source": [
    "### 4.3.3. Configurações para Predição em Base de Dados de Redução de Escala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299edfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64200 entries, 0 to 64199\n",
      "Data columns (total 12 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Unnamed: 0       64200 non-null  int64  \n",
      " 1   lat              64200 non-null  float64\n",
      " 2   lon              64200 non-null  float64\n",
      " 3   ano              64200 non-null  int64  \n",
      " 4   mes              64200 non-null  int64  \n",
      " 5   pr_local         18000 non-null  float64\n",
      " 6   pnt              64200 non-null  object \n",
      " 7   pr               64200 non-null  float64\n",
      " 8   IDW              64200 non-null  float64\n",
      " 9   pr_mes_anterior  64200 non-null  float64\n",
      " 10  pr_acum_6m       64200 non-null  float64\n",
      " 11  cluster          64200 non-null  int32  \n",
      "dtypes: float64(7), int32(1), int64(3), object(1)\n",
      "memory usage: 5.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Abrindo base de dados para configuração\n",
    "df_aesa_to_cnrm_cm6_1hr = pd.read_csv(f'../datas/interim/4.3.2_create_downscaling_database/aesa_to_cnrm_cm6_1hr_{database_type}_downscaling.csv')\n",
    "\n",
    "# Adicionando coluna de atraso de um mes da precipitação\n",
    "df_aesa_to_cnrm_cm6_1hr['pr_mes_anterior'] = df_aesa_to_cnrm_cm6_1hr.sort_values(by=['pnt', 'ano', 'mes']).groupby('pnt')['pr'].shift(1)\n",
    "df_aesa_to_cnrm_cm6_1hr['pr_mes_anterior'] = df_aesa_to_cnrm_cm6_1hr['pr_mes_anterior'].fillna(df_aesa_to_cnrm_cm6_1hr['pr_mes_anterior'].mean())\n",
    "\n",
    "# Adicionando coluna de atraso de acumulado de seis meses de precipitações anteriores\n",
    "df_aesa_to_cnrm_cm6_1hr['pr_acum_6m'] = df_aesa_to_cnrm_cm6_1hr.sort_values(['pnt', 'ano', 'mes']).groupby('pnt')['pr'].rolling(window=6).sum().reset_index(0, drop=True)\n",
    "df_aesa_to_cnrm_cm6_1hr['pr_acum_6m'] = df_aesa_to_cnrm_cm6_1hr['pr_acum_6m'].fillna(df_aesa_to_cnrm_cm6_1hr['pr_acum_6m'].mean())\n",
    "\n",
    "# Adicionando coluna de agrupamento de dados\n",
    "df_coords = df_aesa_to_cnrm_cm6_1hr[['lat', 'lon']].drop_duplicates()\n",
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(df_coords)\n",
    "df_coords['cluster'] = kmeans.labels_\n",
    "df_aesa_to_cnrm_cm6_1hr = df_aesa_to_cnrm_cm6_1hr.merge(df_coords, on=['lat', 'lon'], how='left')\n",
    "\n",
    "# Salvando nova base de dados gerada\n",
    "if databases_generate:  \n",
    "    df_aesa_to_cnrm_cm6_1hr.to_csv(f'../datas/interim/4.3.3_finish_downscaling_database/aesa_to_cnrm_cm6_1hr_{database_type}_downscaling_complete.csv')\n",
    "\n",
    "# Limpando avisos\n",
    "clear()\n",
    "\n",
    "# Informações da base de dados com novas features\n",
    "df_aesa_to_cnrm_cm6_1hr.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5746116c",
   "metadata": {},
   "source": [
    "### 4.3.4. Geração de Base de Dados de Redução de Escala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccf6f871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análise de Modelos:\n",
      "\n",
      "KNe \t Média r: 0.7088 \t Média RMSE: 64.2264\n",
      "Lin \t Média r: 0.6400 \t Média RMSE: 68.1964\n",
      "CNN \t Média r: 0.7629 \t Média RMSE: 56.7021\n",
      "MLP \t Média r: 0.7405 \t Média RMSE: 59.3355\n",
      "\n",
      "O melhor modelo é: CNN\n"
     ]
    }
   ],
   "source": [
    "# Media Geral\n",
    "\n",
    "# Abrindo base de dados para predição\n",
    "df_aesa_to_cnrm_cm6_1hr = pd.read_csv(f'../datas/interim/4.3.3_finish_downscaling_database/aesa_to_cnrm_cm6_1hr_{database_type}_downscaling_complete.csv')\n",
    "\n",
    "# Colunas X e y\n",
    "X_col, y_col = ['pr', 'pr_acum_6m', 'pr_mes_anterior', 'cluster', 'ano', 'mes', 'lat', 'lon', 'IDW'], \"pr_local\"\n",
    "\n",
    "# if idw_method == True:\n",
    "#     X_col.append('IDW')\n",
    "\n",
    "# Definindo ano que separará o treino e a predição\n",
    "anos_X = [\n",
    "    (1994, 2017), (1994, 2018), (1994, 2019), (1994, 2020), (1994, 2021),\n",
    "]\n",
    "anos_Y = [\n",
    "    (2018, 2019), (2019, 2020), (2020, 2021), (2021, 2022), (2022, 2023),\n",
    "]\n",
    "\n",
    "# Supondo que sua função de predição já esteja definida:\n",
    "\n",
    "models = predicao_por_ml(df_aesa_to_cnrm_cm6_1hr[df_aesa_to_cnrm_cm6_1hr['ano'] <= 2023], X_col, y_col, anos_X, anos_Y)\n",
    "models.append(predicao_por_cnn(df_aesa_to_cnrm_cm6_1hr[df_aesa_to_cnrm_cm6_1hr['ano'] <= 2023], X_col, y_col, anos_X, anos_Y, 29))\n",
    "models.append(predicao_por_mlp(df_aesa_to_cnrm_cm6_1hr[df_aesa_to_cnrm_cm6_1hr['ano'] <= 2023], X_col, y_col, anos_X, anos_Y, 29))\n",
    "\n",
    "print('Análise de Modelos:\\n')\n",
    "\n",
    "best_model, r = '', 0\n",
    "for model in models:\n",
    "\n",
    "    print(f\"{model[0][:3]} \\t Média r: {model[1]:.4f} \\t Média RMSE: {model[2]:.4f}\")\n",
    "\n",
    "    if r <= model[1]:\n",
    "        best_model = model[0]\n",
    "        r          = model[1]\n",
    "\n",
    "print(f'\\nO melhor modelo é: {best_model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "363ffb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O r vai de 0.4870 à 0.8379, enquanto o RMSE vai de 43.0124 à 101.8926.\n"
     ]
    }
   ],
   "source": [
    "# Ponto a ponto\n",
    "\n",
    "pontos_vizinhos = [\n",
    "    [\n",
    "        (-6.241325457102246, -39.0),\n",
    "        (-6.740631489359108, -39.0),\n",
    "        (-7.2399375206127985, -39.0),\n",
    "        (-6.241325457102246, -38.5),\n",
    "        (-6.740631489359108, -38.5)\n",
    "    ],\n",
    "    [\n",
    "        (-7.73924355078599, -39.0),\n",
    "        (-8.238549579800534, -39.0),\n",
    "        (-7.2399375206127985, -38.5),\n",
    "        (-7.73924355078599, -38.5),\n",
    "        (-8.238549579800534, -38.5)\n",
    "    ],\n",
    "    [\n",
    "        (-6.241325457102246, -38.0),\n",
    "        (-6.740631489359108, -38.0),\n",
    "        (-7.2399375206127985, -38.0),\n",
    "        (-6.241325457102246, -37.5),\n",
    "        (-6.740631489359108, -37.5)\n",
    "    ],\n",
    "    [\n",
    "        (-7.73924355078599, -38.0),\n",
    "        (-8.238549579800534, -38.0),\n",
    "        (-7.2399375206127985, -37.5),\n",
    "        (-7.73924355078599, -37.5),\n",
    "        (-8.238549579800534, -37.5)\n",
    "    ],\n",
    "    [\n",
    "        (-6.241325457102246, -37.0),\n",
    "        (-6.740631489359108, -37.0),\n",
    "        (-7.2399375206127985, -37.0),\n",
    "        (-6.241325457102246, -36.5),\n",
    "        (-6.740631489359108, -36.5)\n",
    "    ],\n",
    "    [\n",
    "        (-7.73924355078599, -37.0),\n",
    "        (-8.238549579800534, -37.0),\n",
    "        (-7.2399375206127985, -36.5),\n",
    "        (-7.73924355078599, -36.5),\n",
    "        (-8.238549579800534, -36.5)\n",
    "    ],\n",
    "    [\n",
    "        (-6.241325457102246, -36.0),\n",
    "        (-6.740631489359108, -36.0),\n",
    "        (-7.2399375206127985, -36.0),\n",
    "        (-6.241325457102246, -35.5),\n",
    "        (-6.740631489359108, -35.5)\n",
    "    ],\n",
    "    [\n",
    "        (-7.73924355078599, -36.0),\n",
    "        (-8.238549579800534, -36.0),\n",
    "        (-7.2399375206127985, -35.5),\n",
    "        (-7.73924355078599, -35.5),\n",
    "        (-8.238549579800534, -35.5)\n",
    "    ],\n",
    "    [\n",
    "        (-6.241325457102246, -35.0),\n",
    "        (-6.740631489359108, -35.0),\n",
    "        (-7.2399375206127985, -35.0),\n",
    "        (-6.241325457102246, -34.5),\n",
    "        (-6.740631489359108, -34.5)\n",
    "    ],\n",
    "    [\n",
    "        (-7.73924355078599, -35.0),\n",
    "        (-8.238549579800534, -35.0),\n",
    "        (-7.2399375206127985, -34.5),\n",
    "        (-7.73924355078599, -34.5),\n",
    "        (-8.238549579800534, -34.5)\n",
    "    ],\n",
    "]\n",
    "\n",
    "r_max, r_min, rmse_max, rmse_min = 0, 1, 0, 100\n",
    "\n",
    "for i in range(len(pontos_vizinhos)):\n",
    "   \n",
    "    df_ = df_aesa_to_cnrm_cm6_1hr[df_aesa_to_cnrm_cm6_1hr[['lat', 'lon']].apply(tuple, axis=1).isin(set(pontos_vizinhos[i]))]\n",
    "\n",
    "    X_col, y_col = ['pr', 'pr_acum_6m', 'pr_mes_anterior', 'cluster', 'ano', 'mes', 'IDW'], \"pr_local\"\n",
    "\n",
    "    model = predicao_por_cnn(df_[df_['ano'] <= 2023], X_col, y_col, anos_X, anos_Y, 29)\n",
    "\n",
    "    if model[1] > r_max:\n",
    "        r_max = model[1]\n",
    "    \n",
    "    if model[1] < r_min:\n",
    "        r_min = model[1]\n",
    "\n",
    "    if model[2] > rmse_max:\n",
    "        rmse_max = model[2]\n",
    "    \n",
    "    if model[2] < rmse_min:\n",
    "        rmse_min = model[2]\n",
    "\n",
    "print(f\"O r vai de {r_min:.4f} à {r_max:.4f}, enquanto o RMSE vai de {rmse_min:.4f} à {rmse_max:.4f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de92ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 886us/step\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64200 entries, 0 to 64199\n",
      "Data columns (total 13 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Unnamed: 0.1     64200 non-null  int64  \n",
      " 1   Unnamed: 0       64200 non-null  int64  \n",
      " 2   lat              64200 non-null  float64\n",
      " 3   lon              64200 non-null  float64\n",
      " 4   ano              64200 non-null  int64  \n",
      " 5   mes              64200 non-null  int64  \n",
      " 6   pr_local         64200 non-null  float64\n",
      " 7   pnt              64200 non-null  object \n",
      " 8   pr               64200 non-null  float64\n",
      " 9   IDW              64200 non-null  float64\n",
      " 10  pr_mes_anterior  64200 non-null  float64\n",
      " 11  pr_acum_6m       64200 non-null  float64\n",
      " 12  cluster          64200 non-null  int64  \n",
      "dtypes: float64(7), int64(5), object(1)\n",
      "memory usage: 6.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Abrindo base de dados para predição\n",
    "df_aesa_to_cnrm_cm6_1hr = pd.read_csv(f'../datas/interim/4.3.3_finish_downscaling_database/aesa_to_cnrm_cm6_1hr_{database_type}_downscaling_complete.csv')\n",
    "\n",
    "# Fixando seed\n",
    "fix_seed(58)\n",
    "\n",
    "# Configurando base de dados\n",
    "X_train = df_aesa_to_cnrm_cm6_1hr[df_aesa_to_cnrm_cm6_1hr['ano'] <= 2023][X_col].values\n",
    "y_train = df_aesa_to_cnrm_cm6_1hr[df_aesa_to_cnrm_cm6_1hr['ano'] <= 2023][y_col].values\n",
    "\n",
    "# Normalização\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Redimensionar para 3D: (samples, timesteps=1, features)\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "\n",
    "# Criando modelo CNN\n",
    "cnn = Sequential([\n",
    "    Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Conv1D(64, kernel_size=3, padding='same', activation='relu'),\n",
    "    MaxPooling1D(1),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "cnn.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mse'])\n",
    "\n",
    "# Treinamento\n",
    "cnn.fit(X_train, y_train, epochs=5, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Salvando modelo\n",
    "if databases_generate:\n",
    "    cnn.save('../models/statistical_downscaling.h5')\n",
    "\n",
    "# Configurando base de dados para predição\n",
    "X_pr_local = df_aesa_to_cnrm_cm6_1hr[df_aesa_to_cnrm_cm6_1hr['ano'] > 2023][X_col].values\n",
    "\n",
    "# Normalizando dados de predição\n",
    "X_pr_local = scaler.transform(X_pr_local)\n",
    "\n",
    "# Redefinindo escala dos dados\n",
    "X_pr_local = X_pr_local.reshape((X_pr_local.shape[0], 1, X_pr_local.shape[1]))\n",
    "\n",
    "# Previsão dos valores de 'pr' com o modelo treinado\n",
    "pr_local = cnn.predict(X_pr_local).flatten()\n",
    "\n",
    "# Adicionando a nova coluna 'pr' ao DataFrame\n",
    "df_aesa_to_cnrm_cm6_1hr.loc[df_aesa_to_cnrm_cm6_1hr[y_col].isnull(), y_col] = pr_local\n",
    "\n",
    "# Exportando base de dados para CSV\n",
    "if databases_generate:\n",
    "    df_aesa_to_cnrm_cm6_1hr.to_csv(f'../datas/processed/4.3.4_downscaling_database/aesa_to_cnrm_cm6_1hr_{database_type}_downscaling_database.csv')\n",
    "\n",
    "# Informações da base de dados predita\n",
    "df_aesa_to_cnrm_cm6_1hr.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc804a13",
   "metadata": {},
   "source": [
    "### 4.3.5. Geração de Modelo Preditivo Final de Interpolação de Base de Dados de Redução de Escala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f45cbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificação de Modelos:\n",
      "\n",
      "Modelo: Ext \t r: 0.9839 \t RMSE: 15.4709\n",
      "Modelo: Ran \t r: 0.9779 \t RMSE: 18.0213\n",
      "Modelo: Lin \t r: 0.7936 \t RMSE: 52.4367\n",
      "\n",
      "O melhor modelo de ML para a base de dados é: ExtraTrees.\n"
     ]
    }
   ],
   "source": [
    "df_aesa_to_cnrm_cm6_1hr = pd.read_csv(f'../datas/processed/4.3.4_downscaling_database/aesa_to_cnrm_cm6_1hr_{database_type}_downscaling_database.csv')\n",
    "\n",
    "# Colunas X e y\n",
    "X_col, y_col = ['ano', 'mes', 'lat', 'lon'], \"pr_local\"\n",
    "\n",
    "# Escolhendo melhor modelo preditivo\n",
    "model = interpolacao_por_ml(df_aesa_to_cnrm_cm6_1hr, X_col, y_col, \"pnt\", 5)\n",
    "\n",
    "# Definindo features (X) e variável alvo (y)\n",
    "X = df_aesa_to_cnrm_cm6_1hr[X_col].copy()\n",
    "y = df_aesa_to_cnrm_cm6_1hr[y_col].copy()\n",
    "\n",
    "# Treinamento\n",
    "model.fit(X, y)\n",
    "\n",
    "# Salvando o modelo\n",
    "if databases_generate:\n",
    "    joblib.dump(model, f'../models/statistical_downscaling_spatialization.joblib', compress=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
