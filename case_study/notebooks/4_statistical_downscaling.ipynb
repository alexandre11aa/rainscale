{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c897ee",
   "metadata": {},
   "source": [
    "# 4. Downscaling de Dados Locais e Dados do CMIP6\n",
    "\n",
    "```python\n",
    "Esse caderno tem como objetivo a obtenção da precipitação futura de dados locais \n",
    "para os pontos definidos nos GCMs do CMIP6 a partir de predição.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1867920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    ExtraTreesRegressor,\n",
    "    GradientBoostingRegressor\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv1D,\n",
    "    MaxPooling1D,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50625fc1",
   "metadata": {},
   "source": [
    "## 4.1. Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82b688c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição de se vai ocorrer ou não a geração de bases de dados\n",
    "databases_generate = True\n",
    "\n",
    "# Definição de se vai ocorrer ou não o a geração e o uso do método IDW\n",
    "idw_method = True\n",
    "idw_generate = False\n",
    "\n",
    "# Tipo de base de dados local utilizada ('sum' ou 'max')\n",
    "database_type = 'sum'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9ce24e",
   "metadata": {},
   "source": [
    "## 4.2. Funções"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae316f3",
   "metadata": {},
   "source": [
    "### 4.2.1. Função para Limeza de Terminal e Células"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "49f72a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear():\n",
    "    '''\n",
    "    Função para limpar terminal ou célula\n",
    "    '''\n",
    "\n",
    "    # Limpando terminal\n",
    "    os.system('cls')\n",
    "\n",
    "    # Limpando célula\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9052cd18",
   "metadata": {},
   "source": [
    "### 4.2.3. Função para ficar Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b34d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_seed(seed):\n",
    "    '''\n",
    "    Fixa seed informada\n",
    "    '''\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd782b8c",
   "metadata": {},
   "source": [
    "### 4.2.2. Predição a partir de Modelos de Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "36ed126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicao_por_ml(df: pd.DataFrame,\n",
    "                    col_de_treino: list[str],\n",
    "                    var_de_predicao: str,\n",
    "                    anos_X: int):\n",
    "\n",
    "    # Definição dos modelos\n",
    "    models = [\n",
    "\n",
    "        # (\"ExtraTrees\", ExtraTreesRegressor(\n",
    "        #     n_estimators=15,\n",
    "        #     max_depth=20,\n",
    "        #     max_features=2,\n",
    "        #     min_samples_split=2,\n",
    "        #     min_samples_leaf=1,\n",
    "        #     random_state=7\n",
    "        # )),\n",
    "\n",
    "        (\"RandomForest\", RandomForestRegressor(\n",
    "            n_estimators=15,\n",
    "            max_depth=25,\n",
    "            max_features=2,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            random_state=7\n",
    "        )),\n",
    "\n",
    "        (\"GradientBoosting\", GradientBoostingRegressor(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=5,\n",
    "            subsample=0.8,\n",
    "            random_state=7\n",
    "        )),\n",
    "\n",
    "        (\"KNeighbors\", KNeighborsRegressor(\n",
    "            n_neighbors=7,\n",
    "            weights='distance',\n",
    "            algorithm='auto'\n",
    "        )),\n",
    "\n",
    "        (\"LinearRegression\", LinearRegression(\n",
    "            fit_intercept=True,\n",
    "            positive=False\n",
    "        ))\n",
    "\n",
    "    ]\n",
    "\n",
    "    best_model = ['', 0]\n",
    "\n",
    "    models_infos = []\n",
    "\n",
    "    for model in models:\n",
    "\n",
    "        # Lista de métricas por modelo\n",
    "        r2_list, rmse_list = [], []\n",
    "\n",
    "        for ano_X in anos_X:\n",
    "\n",
    "            df_treino = df[df['ano'] <= ano_X].copy()\n",
    "            df_teste  = df[df['ano']  > ano_X].copy()\n",
    "\n",
    "            X_train = df_treino[col_de_treino]\n",
    "            y_train = df_treino[var_de_predicao]\n",
    "\n",
    "            X_test = df_teste[col_de_treino]\n",
    "            y_test = df_teste[var_de_predicao]\n",
    "\n",
    "            model[1].fit(X_train, y_train)\n",
    "            y_pred = model[1].predict(X_test)\n",
    "\n",
    "            r2_list.append(r2_score(y_test, y_pred))  # R²\n",
    "            rmse_list.append(np.sqrt(mean_squared_error(y_test, y_pred)))  # RMSE\n",
    "\n",
    "        if np.mean(r2_list) >= best_model[1]:\n",
    "            best_model[0] = model[1]\n",
    "            best_model[1] = np.mean(r2_list)\n",
    "\n",
    "        # print(f\"{model[0][:3]} \\t Média R²: {np.mean(r2_list):.4f} \\t Média RMSE: {np.mean(rmse_list):.4f}\")\n",
    "\n",
    "        models_infos.append((model[0], np.mean(r2_list), np.mean(rmse_list)))\n",
    "\n",
    "    # return (best_model[0], best_model[1])\n",
    "\n",
    "    return models_infos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac1ca00",
   "metadata": {},
   "source": [
    "### 4.2.3. Predição a partir de Modelos de Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicao_por_cnn(df: pd.DataFrame,\n",
    "                     col_de_treino: list[str],\n",
    "                     var_de_predicao: str,\n",
    "                     anos_X: int,\n",
    "                     seed=58):\n",
    "\n",
    "    # Fixando seed\n",
    "    fix_seed(seed)\n",
    "\n",
    "    r2_list, rmse_list = [], []\n",
    "\n",
    "    for ano_X in anos_X:\n",
    "\n",
    "        # Separar dados em treino e teste\n",
    "        df_treino = df[df['ano'] <= ano_X].copy()\n",
    "        df_teste  = df[df['ano']  > ano_X].copy()\n",
    "\n",
    "        X_train = df_treino[col_de_treino].values\n",
    "        y_train = df_treino[var_de_predicao].values\n",
    "\n",
    "        X_test = df_teste[col_de_treino].values\n",
    "        y_test = df_teste[var_de_predicao].values\n",
    "\n",
    "        # Normalização\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Redimensionar para 3D: (samples, timesteps=1, features)\n",
    "        X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "        X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "        # Limpar sessão anterior (importante em loops com Keras)Dropout\n",
    "        clear_session()\n",
    "\n",
    "        # Criando modelo CNN\n",
    "        cnn = Sequential([\n",
    "            Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            Conv1D(64, kernel_size=3, padding='same', activation='relu'),\n",
    "            MaxPooling1D(1),\n",
    "            Flatten(),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "\n",
    "        cnn.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mse'])\n",
    "\n",
    "        # Treinamento\n",
    "        cnn.fit(X_train, y_train, epochs=5, validation_split=0.2, verbose=0)\n",
    "\n",
    "        # Previsão e avaliação\n",
    "        y_pred = cnn.predict(X_test).flatten()\n",
    "\n",
    "        r2_list.append(r2_score(y_test, y_pred))\n",
    "        rmse_list.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "    clear()\n",
    "\n",
    "    # print(f\"\\nCNN \\t Média R²: {np.mean(r2_list):.4f} \\t Média RMSE: {np.mean(rmse_list):.4f}\")\n",
    "\n",
    "    return ('CNN', np.mean(r2_list), np.mean(rmse_list))\n",
    "\n",
    "def predicao_por_mlp(df: pd.DataFrame,\n",
    "                     col_de_treino: list[str],\n",
    "                     var_de_predicao: str,\n",
    "                     anos_X: int,\n",
    "                     seed=58):\n",
    "\n",
    "    # Fixando seed\n",
    "    fix_seed(seed)\n",
    "\n",
    "    r2_list, rmse_list = [], []\n",
    "\n",
    "    for ano_X in anos_X:\n",
    "\n",
    "        # Separar dados em treino e teste\n",
    "        df_treino = df[df['ano'] <= ano_X].copy()\n",
    "        df_teste  = df[df['ano']  > ano_X].copy()\n",
    "\n",
    "        X_train = df_treino[col_de_treino].values\n",
    "        y_train = df_treino[var_de_predicao].values\n",
    "\n",
    "        X_test = df_teste[col_de_treino].values\n",
    "        y_test = df_teste[var_de_predicao].values\n",
    "\n",
    "        # Normalização\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Limpar sessão anterior (importante em loops com Keras)Dropout\n",
    "        clear_session()\n",
    "\n",
    "        # Criando modelo CNN\n",
    "        mlp = Sequential([\n",
    "            Input(shape=(X_train.shape[1],)),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "\n",
    "        mlp.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mse'])\n",
    "\n",
    "        # Treinamento\n",
    "        mlp.fit(X_train, y_train, epochs=5, validation_split=0.2, verbose=0)\n",
    "\n",
    "        # Previsão e avaliação\n",
    "        y_pred = mlp.predict(X_test).flatten()\n",
    "\n",
    "        r2_list.append(r2_score(y_test, y_pred))\n",
    "        rmse_list.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "    clear()\n",
    "\n",
    "    # print(f\"\\nMLP \\t Média R²: {np.mean(r2_list):.4f} \\t Média RMSE: {np.mean(rmse_list):.4f}\")\n",
    "\n",
    "    return ('MLP', np.mean(r2_list), np.mean(rmse_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d055b3",
   "metadata": {},
   "source": [
    "### 4.2.5. Interpolação a partir de Modelos de Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dae822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolacao_por_ml(df: pd.DataFrame,\n",
    "                        col_de_treino: list[str],\n",
    "                        var_de_predicao: str,\n",
    "                        var_de_pontos: str,\n",
    "                        n_de_teste: int):\n",
    "\n",
    "    # Obtendo pontos únicos\n",
    "    pontos_unicos = df[var_de_pontos].unique()\n",
    "\n",
    "    # Definição dos modelos\n",
    "    models = [\n",
    "\n",
    "        (\"ExtraTrees\", ExtraTreesRegressor(\n",
    "            n_estimators=15,\n",
    "            max_depth=20,\n",
    "            max_features=2,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            random_state=7\n",
    "        )),\n",
    "\n",
    "        (\"RandomForest\", RandomForestRegressor(\n",
    "            n_estimators=15,\n",
    "            max_depth=25,\n",
    "            max_features=2,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            random_state=7\n",
    "        )),\n",
    "\n",
    "        # (\"KNeighbors\", KNeighborsRegressor(\n",
    "        #     n_neighbors=7,\n",
    "        #     weights='distance',\n",
    "        #     algorithm='auto'\n",
    "        # )),\n",
    "\n",
    "        (\"GradientBoosting\", GradientBoostingRegressor(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=5,\n",
    "            subsample=0.8,\n",
    "            random_state=7\n",
    "        )),\n",
    "\n",
    "        (\"LinearRegression\", LinearRegression(\n",
    "            fit_intercept=True,\n",
    "            positive=False\n",
    "        ))\n",
    "\n",
    "    ]\n",
    "\n",
    "    # Defininção de lista de métricas\n",
    "    metrics = []\n",
    "    for i in range(len(models)):\n",
    "        metrics.append([[], []])\n",
    "\n",
    "    for i in range(n_de_teste):\n",
    "\n",
    "        # Embaralha os pontos únicos\n",
    "        np.random.shuffle(pontos_unicos)\n",
    "\n",
    "        # Dividindo em 70% treino e 30% teste\n",
    "        split_idx = int(len(pontos_unicos) * 0.8)\n",
    "        pontos_treino = set(pontos_unicos[:split_idx])\n",
    "        pontos_teste = set(pontos_unicos[split_idx:])\n",
    "\n",
    "        # Criando DataFrames de treino e teste\n",
    "        df_treino = df[df[var_de_pontos].isin(pontos_treino)].copy()\n",
    "        df_teste = df[df[var_de_pontos].isin(pontos_teste)].copy()\n",
    "\n",
    "        # Definindo features (X) e variável alvo (y)\n",
    "        X_train = df_treino[col_de_treino]\n",
    "        y_train = df_treino[var_de_predicao]\n",
    "\n",
    "        X_test = df_teste[col_de_treino]\n",
    "        y_test = df_teste[var_de_predicao]\n",
    "\n",
    "        # Treinar e avaliar cada modelo\n",
    "        for j in range(len(models)):\n",
    "            models[j][1].fit(X_train, y_train)                                 # Treinamento\n",
    "            y_pred = models[j][1].predict(X_test)                              # Previsão\n",
    "            metrics[j][0].append(r2_score(y_test, y_pred))                     # R²\n",
    "            metrics[j][1].append(np.sqrt(mean_squared_error(y_test, y_pred)))  # RMSE\n",
    "\n",
    "    # Verificando melhor Modelo a partir de r2\n",
    "    best_model = (0, '', '')\n",
    "\n",
    "    print('Verificação de Modelos:\\n')\n",
    "\n",
    "    for i in range(len(metrics)):\n",
    "\n",
    "        r2, rmse = np.mean(metrics[i][0]), np.mean(metrics[i][1])\n",
    "\n",
    "        if best_model[0] < r2:\n",
    "            best_model = r2, models[i][0], models[i][1]\n",
    "\n",
    "        print(f'Modelo: {models[i][0][:3]} \\t R²: {r2:.4f} \\t RMSE: {rmse:.4f}')\n",
    "\n",
    "    print(f'\\nO melhor modelo de ML para a base de dados é: {best_model[1]}.')\n",
    "\n",
    "    return best_model[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322fbd1b",
   "metadata": {},
   "source": [
    "### 4.2.6. Função que Adiciona coluna IDW à Base de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "243b880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def porcentagem_em_barra(valor_atual: int,\n",
    "                         valor_total: int) -> str:\n",
    "    \"\"\"\n",
    "    Gerador de barra de porcentagem a partir de valor atual e total.\n",
    "    \"\"\"\n",
    "\n",
    "    porcentagem = 100 * (valor_atual / valor_total)\n",
    "\n",
    "    completo   = '-' * (int(porcentagem))\n",
    "    incompleto = '_' * (100 - int(porcentagem))\n",
    "\n",
    "    situacao = f'[{completo}{incompleto}] {porcentagem:.2f}% ({valor_atual} de {valor_total})'\n",
    "\n",
    "    return situacao\n",
    "\n",
    "def vizinhos_proximos(lat: float,\n",
    "                      lon: float,\n",
    "                      lat_lon: list[str],\n",
    "                      n_vizinhos: int = 5):\n",
    "    \"\"\"\n",
    "    Gerador de lista de pontos vizinhos próximos de determinado ponto.\n",
    "\n",
    "    Args:\n",
    "        lat (float): Latitude do ponto principal;\n",
    "        lon (float): Logitude do ponto principal;\n",
    "        lat_lon (list[str]): Lista com latitudes e longitudes de pontos próximos ao ponto principal;\n",
    "        n_vizinhos (int, optional): Números de pontos vizinhos ao ponto principal a se estimar.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de n pontos mais próximos ao ponto principal.\n",
    "    \"\"\"\n",
    "\n",
    "    # Lista para armazenar tuplas (string_original, distancia)\n",
    "    distancias = []\n",
    "\n",
    "    for ponto_str in lat_lon:\n",
    "        lat_p, lon_p = map(float, ponto_str.split(\";\"))\n",
    "        dist = np.linalg.norm(np.array([lat, lon]) - np.array([lat_p, lon_p]))\n",
    "        distancias.append((ponto_str, dist))\n",
    "\n",
    "    # Ordena pela menor distância\n",
    "    distancias_ordenadas = sorted(distancias, key=lambda x: x[1])\n",
    "\n",
    "    # Pega os n vizinhos mais próximos (ignorando o primeiro se for o próprio ponto)\n",
    "    vizinhos = [p[0] for p in distancias_ordenadas if p[1] != 0][:n_vizinhos]\n",
    "\n",
    "    return vizinhos\n",
    "\n",
    "def interpolacao_por_idw(df: pd.DataFrame,\n",
    "                         var_de_predicao: str,\n",
    "                         var_de_anos: str,\n",
    "                         var_de_meses: str,\n",
    "                         var_de_pontos: str,\n",
    "                         pontos: str = 'all',\n",
    "                         progresso: bool = True,\n",
    "                         constante: int = 2,\n",
    "                         n_vizinhos: int = 5) -> list:\n",
    "    '''\n",
    "    Interpola dados de séries temporais a partir do método IDW, e cria uma coluna para tal\n",
    "    '''\n",
    "\n",
    "    # Definindo nova coluna para o IDW\n",
    "    df.loc[:, 'IDW'] = np.nan\n",
    "\n",
    "    # Obtendo pontos únicos\n",
    "    if pontos == 'all':\n",
    "        pontos_unicos = df[var_de_pontos].unique()\n",
    "    else:\n",
    "        pontos_unicos = [pontos]\n",
    "\n",
    "    # Varendo pontos únicos\n",
    "    for i, ponto in enumerate(pontos_unicos):\n",
    "\n",
    "        if progresso == True:\n",
    "\n",
    "            # Ponto em cálculo\n",
    "            print(porcentagem_em_barra(i+1, len(pontos_unicos)))\n",
    "\n",
    "        lat, lon = map(float, ponto.split(\";\"))\n",
    "\n",
    "        # Obtendo anos únicos\n",
    "        anos_unicos = df[df[var_de_pontos] == ponto][var_de_anos].unique()\n",
    "\n",
    "        # Varendo anos únicos dos pontos únicos\n",
    "        for ano in anos_unicos:\n",
    "\n",
    "            # Obtendo meses únicos\n",
    "            meses_unicos = df[(df[var_de_pontos] == ponto) &\n",
    "                              (df[var_de_anos] == ano)][var_de_meses].unique()\n",
    "\n",
    "            # Varrendo meses únicos dos anos únicos dos pontos únicos\n",
    "            for mes in meses_unicos:\n",
    "\n",
    "                # Filtrando pontos unicos que possuem mesmo mes e ano que o ponto em varredura\n",
    "                pontos_unicos_filtrados = df[(df[var_de_anos] == ano) &\n",
    "                                             (df[var_de_meses] == mes)][var_de_pontos].unique()\n",
    "\n",
    "                # Obtendo pontos vizinhos mais próximos do ponto em varredura\n",
    "                vizinhos = vizinhos_proximos(lat, lon, pontos_unicos_filtrados, n_vizinhos)\n",
    "\n",
    "                # Definindo variáveis para calcular IDW\n",
    "                dividendo = divisor = 0\n",
    "\n",
    "                for ponto_vizinho in vizinhos:\n",
    "\n",
    "                    lat_vizinha, lon_vizinha = map(float, ponto_vizinho.split(\";\"))\n",
    "\n",
    "                    distancia = ((lat - lat_vizinha)**2 + (lon - lon_vizinha)**2)**(1/2)\n",
    "\n",
    "                    variavel = df.loc[(df[var_de_pontos] == ponto_vizinho) &\n",
    "                                      (df[var_de_anos] == ano) &\n",
    "                                      (df[var_de_meses] == mes), var_de_predicao]\n",
    "\n",
    "                    if not variavel.empty:\n",
    "                        valor = variavel.iloc[0]\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                    dividendo += valor / (distancia**constante)\n",
    "\n",
    "                    divisor += 1 / (distancia**constante)\n",
    "\n",
    "                if divisor == 0:\n",
    "                    idw = np.nan\n",
    "                else:\n",
    "                    idw = dividendo / divisor\n",
    "\n",
    "                idx = df.loc[(df[var_de_pontos] == ponto) &\n",
    "                             (df[var_de_anos] == ano) &\n",
    "                             (df[var_de_meses] == mes), 'IDW'].index[0]\n",
    "\n",
    "                df.at[idx, 'IDW'] = idw\n",
    "\n",
    "        clear()\n",
    "\n",
    "    df['IDW'] = df['IDW'].fillna(df['IDW'].mean())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1db9db",
   "metadata": {},
   "source": [
    "## 4.3. CNRM-CM6-1HR\n",
    "\n",
    "Redução de escala de dados locais e dados experimentais do CMIP6 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f7f7de",
   "metadata": {},
   "source": [
    "### 4.3.1. Coluna IDW para dados CNRM-CM6-1HR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a15e485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Informações do GCM:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64200 entries, 0 to 64199\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   lat     64200 non-null  float64\n",
      " 1   lon     64200 non-null  float64\n",
      " 2   ano     64200 non-null  int64  \n",
      " 3   mes     64200 non-null  int64  \n",
      " 4   pr      64200 non-null  float64\n",
      " 5   pnt     64200 non-null  object \n",
      "dtypes: float64(3), int64(2), object(1)\n",
      "memory usage: 2.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Definindo base de dados de GCM\n",
    "df_cnrm_cm6_1hr = pd.read_csv(\n",
    "    f\"../datas/interim/1.3.2_cmip6_database_create/pr_day_CNRM-CM6-1-HR_ssp585_r1i1p1f2_gr_19940101-21001231_{database_type}.csv\"\n",
    ")\n",
    "\n",
    "# Padronizando valores de longitude\n",
    "df_cnrm_cm6_1hr[\"lon\"] = df_cnrm_cm6_1hr[\"lon\"] - 360\n",
    "\n",
    "# Adicionando coluna de pontos\n",
    "df_cnrm_cm6_1hr['pnt'] = df_cnrm_cm6_1hr[\"lat\"].astype(str) + \";\" + df_cnrm_cm6_1hr[\"lon\"].astype(str)\n",
    "\n",
    "# Caso não tenha a coluna IDW nas \"colunas_de_interesse\", calcula-se o IDW\n",
    "if idw_method == True:\n",
    "\n",
    "    if idw_generate == True:  # 115m 33.8s\n",
    "\n",
    "        df_cnrm_cm6_1hr = interpolacao_por_idw(df_cnrm_cm6_1hr, \"pr\", \"ano\", \"mes\", \"pnt\")\n",
    "        df_cnrm_cm6_1hr.to_csv(f'../datas/interim/4.3.1_cmip6_with_idw/pr_day_CNRM-CM6-1-HR_ssp585_r1i1p1f2_gr_19940101-21001231_{database_type}_idw.csv')\n",
    "\n",
    "    else:\n",
    "\n",
    "        df_cnrm_cm6_1hr = pd.read_csv(f'../datas/interim/4.3.1_cmip6_with_idw/pr_day_CNRM-CM6-1-HR_ssp585_r1i1p1f2_gr_19940101-21001231_{database_type}_idw.csv')\n",
    "\n",
    "# Definindo colunas de interesse\n",
    "colunas_gcm = ['lat', 'lon', 'ano', 'mes', 'pr', 'pnt']\n",
    "\n",
    "# Restringindo base de dados às colunas de interesse\n",
    "df_cnrm_cm6_1hr = df_cnrm_cm6_1hr[colunas_gcm].copy()\n",
    "\n",
    "# Visualizando Bases de Dados do GCM\n",
    "print('- Informações do GCM:')\n",
    "print(df_cnrm_cm6_1hr.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ee1664",
   "metadata": {},
   "source": [
    "### 4.3.2. Criação de Bases de Dados de Redução de Escala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4186b7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18000 entries, 0 to 17999\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   lat       18000 non-null  float64\n",
      " 1   lon       18000 non-null  float64\n",
      " 2   ano       18000 non-null  int64  \n",
      " 3   mes       18000 non-null  int64  \n",
      " 4   pr_local  18000 non-null  float64\n",
      " 5   pnt       18000 non-null  object \n",
      "dtypes: float64(3), int64(2), object(1)\n",
      "memory usage: 843.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Importando base de dados\n",
    "df_aesa_to_cnrm_cm6_1hr = pd.read_csv(f'../datas/interim/3.3.3_aesa_interpolated_to_cmip6/aesa_to_cnrm_cm6_1hr_{database_type}_interpolated.csv')\n",
    "\n",
    "# Definindo colunas de interesse\n",
    "colunas_local = ['lat', 'lon', 'ano', 'mes', 'pr_local', 'pnt']\n",
    "\n",
    "# Adicionando coluna de pontos\n",
    "df_aesa_to_cnrm_cm6_1hr['pnt'] = df_aesa_to_cnrm_cm6_1hr[\"lat\"].astype(str) + \";\" + df_aesa_to_cnrm_cm6_1hr[\"lon\"].astype(str)\n",
    "\n",
    "# Restringindo base de dados às colunas de interesse\n",
    "df_aesa_to_cnrm_cm6_1hr = df_aesa_to_cnrm_cm6_1hr[colunas_local].copy()\n",
    "\n",
    "# Observando informações da base de dados\n",
    "df_aesa_to_cnrm_cm6_1hr.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5971b401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64200 entries, 0 to 64199\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   lat       64200 non-null  float64\n",
      " 1   lon       64200 non-null  float64\n",
      " 2   ano       64200 non-null  int64  \n",
      " 3   mes       64200 non-null  int64  \n",
      " 4   pr_local  18000 non-null  float64\n",
      " 5   pnt       64200 non-null  object \n",
      " 6   pr        64200 non-null  float64\n",
      "dtypes: float64(4), int64(2), object(1)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Fazendo o merge com base em 'pnt', 'ano' e 'mes' para base de dados única\n",
    "df_aesa_to_cnrm_cm6_1hr = df_aesa_to_cnrm_cm6_1hr.merge(df_cnrm_cm6_1hr, on=['pnt', 'ano', 'mes', 'lat', 'lon'], how='outer')\n",
    "\n",
    "# Salvando nova base de dados\n",
    "df_aesa_to_cnrm_cm6_1hr.to_csv(f'../datas/interim/4.3.2_create_downscaling_database/aesa_to_cnrm_cm6_1hr_{database_type}_downscaling.csv')\n",
    "\n",
    "# Informações da base de dados única\n",
    "df_aesa_to_cnrm_cm6_1hr.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502ad727",
   "metadata": {},
   "source": [
    "### 4.3.3. Configurações para Predição em Base de Dados de Redução de Escala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "299edfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64200 entries, 0 to 64199\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Unnamed: 0       64200 non-null  int64  \n",
      " 1   lat              64200 non-null  float64\n",
      " 2   lon              64200 non-null  float64\n",
      " 3   ano              64200 non-null  int64  \n",
      " 4   mes              64200 non-null  int64  \n",
      " 5   pr_local         18000 non-null  float64\n",
      " 6   pnt              64200 non-null  object \n",
      " 7   pr               64200 non-null  float64\n",
      " 8   pr_mes_anterior  64200 non-null  float64\n",
      " 9   pr_acum_6m       64200 non-null  float64\n",
      " 10  cluster          64200 non-null  int32  \n",
      "dtypes: float64(6), int32(1), int64(3), object(1)\n",
      "memory usage: 5.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Abrindo base de dados para configuração\n",
    "df_aesa_to_cnrm_cm6_1hr = pd.read_csv(f'../datas/interim/4.3.2_create_downscaling_database/aesa_to_cnrm_cm6_1hr_{database_type}_downscaling.csv')\n",
    "\n",
    "# Adicionando coluna de atraso de um mes da precipitação\n",
    "df_aesa_to_cnrm_cm6_1hr['pr_mes_anterior'] = df_aesa_to_cnrm_cm6_1hr.sort_values(by=['pnt', 'ano', 'mes']).groupby('pnt')['pr'].shift(1)\n",
    "df_aesa_to_cnrm_cm6_1hr['pr_mes_anterior'] = df_aesa_to_cnrm_cm6_1hr['pr_mes_anterior'].fillna(df_aesa_to_cnrm_cm6_1hr['pr_mes_anterior'].mean())\n",
    "\n",
    "# Adicionando coluna de atraso de acumulado de seis meses de precipitações anteriores\n",
    "df_aesa_to_cnrm_cm6_1hr['pr_acum_6m'] = df_aesa_to_cnrm_cm6_1hr.sort_values(['pnt', 'ano', 'mes']).groupby('pnt')['pr'].rolling(window=6).sum().reset_index(0, drop=True)\n",
    "df_aesa_to_cnrm_cm6_1hr['pr_acum_6m'] = df_aesa_to_cnrm_cm6_1hr['pr_acum_6m'].fillna(df_aesa_to_cnrm_cm6_1hr['pr_acum_6m'].mean())\n",
    "\n",
    "# Adicionando coluna de agrupamento de dados\n",
    "df_coords = df_aesa_to_cnrm_cm6_1hr[['lat', 'lon']].drop_duplicates()\n",
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(df_coords)\n",
    "df_coords['cluster'] = kmeans.labels_\n",
    "df_aesa_to_cnrm_cm6_1hr = df_aesa_to_cnrm_cm6_1hr.merge(df_coords, on=['lat', 'lon'], how='left')\n",
    "\n",
    "# Salvando nova base de dados gerada\n",
    "df_aesa_to_cnrm_cm6_1hr.to_csv(f'../datas/interim/4.3.3_finish_downscaling_database/aesa_to_cnrm_cm6_1hr_{database_type}_downscaling_complete.csv')\n",
    "\n",
    "# Limpando avisos\n",
    "clear()\n",
    "\n",
    "# Informações da base de dados com novas features\n",
    "df_aesa_to_cnrm_cm6_1hr.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5746116c",
   "metadata": {},
   "source": [
    "### 4.3.4. Geração de Base de Dados de Redução de Escala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ccf6f871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análise de Modelos:\n",
      "\n",
      "Ran \t Média R²: 0.5721 \t Média RMSE: 57.6727\n",
      "Gra \t Média R²: 0.5483 \t Média RMSE: 59.0816\n",
      "KNe \t Média R²: 0.4443 \t Média RMSE: 65.4520\n",
      "Lin \t Média R²: 0.3621 \t Média RMSE: 70.4603\n",
      "CNN \t Média R²: 0.5820 \t Média RMSE: 57.0266\n",
      "MLP \t Média R²: 0.5400 \t Média RMSE: 59.8265\n",
      "\n",
      "O melhor modelo é: CNN\n"
     ]
    }
   ],
   "source": [
    "# Abrindo base de dados para predição\n",
    "df_aesa_to_cnrm_cm6_1hr = pd.read_csv(f'../datas/interim/4.3.3_finish_downscaling_database/aesa_to_cnrm_cm6_1hr_{database_type}_downscaling_complete.csv')\n",
    "\n",
    "# Colunas X e y\n",
    "X_col, y_col = ['pr', 'pr_acum_6m', 'pr_mes_anterior', 'cluster', 'ano', 'mes', 'lat', 'lon'], \"pr_local\"\n",
    "\n",
    "# Definindo ano que separará o treino e a predição\n",
    "anos_X = [2016, 2017, 2018, 2019, 2020, 2021, 2022]\n",
    "\n",
    "# Supondo que sua função de predição já esteja definida:\n",
    "\n",
    "models = predicao_por_ml(df_aesa_to_cnrm_cm6_1hr[df_aesa_to_cnrm_cm6_1hr['ano'] <= 2023], X_col, y_col, anos_X)\n",
    "\n",
    "models.append(predicao_por_cnn(df_aesa_to_cnrm_cm6_1hr[df_aesa_to_cnrm_cm6_1hr['ano'] <= 2023], X_col, y_col, anos_X, 58))\n",
    "models.append(predicao_por_mlp(df_aesa_to_cnrm_cm6_1hr[df_aesa_to_cnrm_cm6_1hr['ano'] <= 2023], X_col, y_col, anos_X, 58))\n",
    "\n",
    "print('Análise de Modelos:\\n')\n",
    "\n",
    "best_model, r2 = '', 0\n",
    "for model in models:\n",
    "\n",
    "    print(f\"{model[0][:3]} \\t Média R²: {model[1]:.4f} \\t Média RMSE: {model[2]:.4f}\")\n",
    "\n",
    "    if r2 <= model[1]:\n",
    "        best_model = model[0]\n",
    "        r2         = model[1]\n",
    "\n",
    "print(f'\\nO melhor modelo é: {best_model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1de92ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1444/1444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 967us/step\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64200 entries, 0 to 64199\n",
      "Data columns (total 12 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Unnamed: 0.1     64200 non-null  int64  \n",
      " 1   Unnamed: 0       64200 non-null  int64  \n",
      " 2   lat              64200 non-null  float64\n",
      " 3   lon              64200 non-null  float64\n",
      " 4   ano              64200 non-null  int64  \n",
      " 5   mes              64200 non-null  int64  \n",
      " 6   pr_local         64200 non-null  float64\n",
      " 7   pnt              64200 non-null  object \n",
      " 8   pr               64200 non-null  float64\n",
      " 9   pr_mes_anterior  64200 non-null  float64\n",
      " 10  pr_acum_6m       64200 non-null  float64\n",
      " 11  cluster          64200 non-null  int64  \n",
      "dtypes: float64(6), int64(5), object(1)\n",
      "memory usage: 5.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Abrindo base de dados para predição\n",
    "df_aesa_to_cnrm_cm6_1hr = pd.read_csv(f'../datas/interim/4.3.3_finish_downscaling_database/aesa_to_cnrm_cm6_1hr_{database_type}_downscaling_complete.csv')\n",
    "\n",
    "# Fixando seed\n",
    "fix_seed(58)\n",
    "\n",
    "# Configurando base de dados\n",
    "X_train = df_aesa_to_cnrm_cm6_1hr[df_aesa_to_cnrm_cm6_1hr['ano'] <= 2023][X_col].values\n",
    "y_train = df_aesa_to_cnrm_cm6_1hr[df_aesa_to_cnrm_cm6_1hr['ano'] <= 2023][y_col].values\n",
    "\n",
    "# Normalização\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Redimensionar para 3D: (samples, timesteps=1, features)\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "\n",
    "# Criando modelo CNN\n",
    "cnn = Sequential([\n",
    "    Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Conv1D(64, kernel_size=3, padding='same', activation='relu'),\n",
    "    MaxPooling1D(1),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "cnn.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mse'])\n",
    "\n",
    "# Treinamento\n",
    "cnn.fit(X_train, y_train, epochs=5, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Configurando base de dados para predição\n",
    "X_pr_local = df_aesa_to_cnrm_cm6_1hr[df_aesa_to_cnrm_cm6_1hr['ano'] > 2023][X_col].values\n",
    "\n",
    "# Normalizando dados de predição\n",
    "X_pr_local = scaler.transform(X_pr_local)\n",
    "\n",
    "# Redefinindo escala dos dados\n",
    "X_pr_local = X_pr_local.reshape((X_pr_local.shape[0], 1, X_pr_local.shape[1]))\n",
    "\n",
    "# Previsão dos valores de 'pr' com o modelo treinado\n",
    "pr_local = cnn.predict(X_pr_local).flatten()\n",
    "\n",
    "# Adicionando a nova coluna 'pr' ao DataFrame\n",
    "df_aesa_to_cnrm_cm6_1hr.loc[df_aesa_to_cnrm_cm6_1hr[y_col].isnull(), y_col] = pr_local\n",
    "\n",
    "# Exportando base de dados para CSV\n",
    "df_aesa_to_cnrm_cm6_1hr.to_csv(f'../datas/processed/4.3.4_downscaling_database/aesa_to_cnrm_cm6_1hr_{database_type}_downscaling_database.csv')\n",
    "\n",
    "# Informações da base de dados predita\n",
    "df_aesa_to_cnrm_cm6_1hr.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc804a13",
   "metadata": {},
   "source": [
    "### 4.3.5. Geração de Modelo Preditivo Final de Interpolação de Base de Dados de Redução de Escala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4f45cbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificação de Modelos:\n",
      "\n",
      "Modelo: Ext \t R²: 0.9661 \t RMSE: 14.2691\n",
      "Modelo: Ran \t R²: 0.9568 \t RMSE: 16.1247\n",
      "Modelo: KNe \t R²: 0.9687 \t RMSE: 13.7283\n",
      "Modelo: Gra \t R²: 0.9260 \t RMSE: 21.0915\n",
      "Modelo: Lin \t R²: 0.5498 \t RMSE: 52.0289\n",
      "\n",
      "O melhor modelo de ML para a base de dados é: KNeighbors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../models/aesa_to_cnrm_cm6_1hr_sum_downscaling_database.joblib']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aesa_to_cnrm_cm6_1hr = pd.read_csv(f'../datas/processed/4.3.4_downscaling_database/aesa_to_cnrm_cm6_1hr_{database_type}_downscaling_database.csv')\n",
    "\n",
    "# Colunas X e y\n",
    "X_col, y_col = ['ano', 'mes', 'lat', 'lon'], \"pr_local\"\n",
    "\n",
    "# Escolhendo melhor modelo preditivo\n",
    "model = interpolacao_por_ml(df_aesa_to_cnrm_cm6_1hr, X_col, y_col, \"pnt\", 1)\n",
    "\n",
    "# Definindo features (X) e variável alvo (y)\n",
    "X = df_aesa_to_cnrm_cm6_1hr[X_col].copy()\n",
    "y = df_aesa_to_cnrm_cm6_1hr[y_col].copy()\n",
    "\n",
    "# Treinamento\n",
    "model.fit(X, y)\n",
    "\n",
    "# Salvando o modelo\n",
    "joblib.dump(model, f'../models/aesa_to_cnrm_cm6_1hr_{database_type}_downscaling_database.joblib', compress=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
