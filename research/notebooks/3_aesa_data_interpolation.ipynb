{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd4f9542",
   "metadata": {},
   "source": [
    "# 3. Espacialização de Dados Locais para Pontos do CMIP6\n",
    "\n",
    "```python\n",
    "Esse caderno tem como objetivo a obtenção da precipitação de dados locais \n",
    "para os pontos definidos nos GCMs do CMIP6 a partir de interpolação espacial.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3b45bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from geopy.distance import geodesic\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error\n",
    ")\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    ExtraTreesRegressor,\n",
    "    GradientBoostingRegressor\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv1D,\n",
    "    LSTM,\n",
    "    MaxPooling1D,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cd9789",
   "metadata": {},
   "source": [
    "## 3.1 Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90f094f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição de se vai ocorrer ou não a geração de bases de dados\n",
    "databases_generate = True\n",
    "\n",
    "# Definição de se vai ocorrer ou não o a geração e o uso do método IDW\n",
    "idw_method = True\n",
    "idw_generate = False\n",
    "\n",
    "# Tipo de base de dados local utilizada ('sum' ou 'max')\n",
    "database_type = 'sum'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b73232c",
   "metadata": {},
   "source": [
    "## 3.2 Funções"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f405034d",
   "metadata": {},
   "source": [
    "### 3.1.1. Função para Limeza de Terminal e Células"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0db8776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear():\n",
    "    '''\n",
    "    Função para limpar terminal ou célula\n",
    "    '''\n",
    "\n",
    "    # Limpando terminal\n",
    "    # os.system('cls')\n",
    "\n",
    "    # Limpando célula\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083f245d",
   "metadata": {},
   "source": [
    "### 3.1.2. Função que Adiciona coluna IDW à Base de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d44ddef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def porcentagem_em_barra(valor_atual: int,\n",
    "                         valor_total: int) -> str:\n",
    "    \"\"\"\n",
    "    Gerador de barra de porcentagem a partir de valor atual e total.\n",
    "    \"\"\"\n",
    "\n",
    "    porcentagem = 100 * (valor_atual / valor_total)\n",
    "\n",
    "    completo   = '━' * (int(porcentagem))\n",
    "    incompleto = '╺' * (100 - int(porcentagem))\n",
    "\n",
    "    situacao = f'[{completo}{incompleto}] {porcentagem:.2f}% ({valor_atual} de {valor_total})'\n",
    "\n",
    "    return situacao\n",
    "\n",
    "def idw_interpolation(target_point, neighbors, values, const=2):\n",
    "    weights = []\n",
    "    for pt in neighbors:\n",
    "        dist = geodesic(target_point, pt).meters\n",
    "        if dist == 0:\n",
    "            continue  # Evita usar o próprio ponto\n",
    "        weights.append(1 / (dist ** const))\n",
    "    weights = np.array(weights)\n",
    "    values = np.array(values)\n",
    "    return np.sum(weights * values) / np.sum(weights) if np.sum(weights) > 0 else np.nan\n",
    "\n",
    "def calcular_precipitacao_idw(df: pd.DataFrame,\n",
    "                               var_de_interpolacao: str,\n",
    "                               var_de_latitude: str,\n",
    "                               var_de_longitude: str,\n",
    "                               var_de_anos: str,\n",
    "                               var_de_meses: str,\n",
    "                               n_vizinhos: int = 10,\n",
    "                               const: int = 2) -> pd.DataFrame:\n",
    "\n",
    "    df_resultado = df.copy()\n",
    "    df_resultado[\"IDW\"] = np.nan  # nova coluna para os resultados\n",
    "\n",
    "    total = len(df_resultado)\n",
    "    for i, idx in enumerate(df_resultado.index):\n",
    "\n",
    "        row = df_resultado.loc[idx]\n",
    "        ano, mes, lat, lon = row[var_de_anos], row[var_de_meses], row[var_de_latitude], row[var_de_longitude]\n",
    "        target_point = (lat, lon)\n",
    "\n",
    "        # Candidatos com mesma data, excluindo a própria linha\n",
    "        candidatos = df_resultado[(df_resultado[var_de_anos] == ano) &\n",
    "                                  (df_resultado[var_de_meses] == mes) &\n",
    "                                  (df_resultado.index != idx)].copy()\n",
    "\n",
    "        if candidatos.empty:\n",
    "            continue\n",
    "\n",
    "        # Calcular distâncias\n",
    "        candidatos['distancia'] = candidatos.apply(\n",
    "            lambda r: geodesic(target_point, (r[var_de_latitude], r[var_de_longitude])).meters, axis=1\n",
    "        )\n",
    "\n",
    "        # Selecionar os vizinhos mais próximos\n",
    "        vizinhos = candidatos.nsmallest(n_vizinhos, 'distancia')\n",
    "\n",
    "        if not vizinhos.empty:\n",
    "            viz_points = list(zip(vizinhos[var_de_latitude], vizinhos[var_de_longitude]))\n",
    "            viz_values = vizinhos[var_de_interpolacao].tolist()\n",
    "\n",
    "            interpolado = idw_interpolation(target_point, viz_points, viz_values, const)\n",
    "            df_resultado.at[idx, \"IDW\"] = interpolado\n",
    "\n",
    "        # Barra de progresso a cada 10 registros\n",
    "        if (i + 1) % 10 == 0 or (i + 1) == total:\n",
    "            clear()\n",
    "            print(porcentagem_em_barra(i + 1, total))\n",
    "\n",
    "    return df_resultado\n",
    "\n",
    "def preencher_precipitacao_idw(df: pd.DataFrame,\n",
    "                               var_de_interpolacao: str,\n",
    "                               var_de_latitude: str,\n",
    "                               var_de_longitude: str,\n",
    "                               var_de_anos: str,\n",
    "                               var_de_meses: str,\n",
    "                               n_vizinhos: int = 10,\n",
    "                               const: int = 2) -> pd.DataFrame:\n",
    "\n",
    "    df_resultado = df.copy()\n",
    "\n",
    "    # Iterar sobre os índices com valores faltantes\n",
    "    missing_indices = df_resultado[df_resultado[var_de_interpolacao].isna()].index\n",
    "\n",
    "    valor_atual, valor_total = 1, len(missing_indices)\n",
    "\n",
    "    for idx in missing_indices:\n",
    "\n",
    "        row = df_resultado.loc[idx]\n",
    "        ano, mes, lat, lon = row[var_de_anos], row[var_de_meses], row[var_de_latitude], row[var_de_longitude]\n",
    "        target_point = (lat, lon)\n",
    "\n",
    "        # Filtrar pontos com mesma data e precipitação conhecida\n",
    "        filtro = (df_resultado[var_de_anos] == ano) & (df_resultado[var_de_meses] == mes) & df_resultado[var_de_interpolacao].notna()\n",
    "        candidatos = df_resultado[filtro].copy()\n",
    "\n",
    "        # Calcular distâncias\n",
    "        candidatos['distancia'] = candidatos.apply(lambda r: geodesic(target_point, (r[var_de_latitude], r[var_de_longitude])).meters, axis=1)\n",
    "\n",
    "        # Selecionar os pontos mais próximos\n",
    "        vizinhos = candidatos.nsmallest(n_vizinhos, 'distancia')\n",
    "\n",
    "        if not vizinhos.empty:\n",
    "            viz_points = list(zip(vizinhos[var_de_latitude], vizinhos[var_de_longitude]))\n",
    "            viz_values = vizinhos[var_de_interpolacao].tolist()\n",
    "\n",
    "            # Aplicar IDW\n",
    "            interpolado = idw_interpolation(target_point, viz_points, viz_values, const)\n",
    "            df_resultado.at[idx, var_de_interpolacao] = interpolado\n",
    "\n",
    "        if valor_atual % 10 == 0 or valor_atual == valor_total:\n",
    "            clear()\n",
    "            print(porcentagem_em_barra(valor_atual, valor_total))\n",
    "\n",
    "        valor_atual += 1\n",
    "\n",
    "    return df_resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6709bccb",
   "metadata": {},
   "source": [
    "### 3.1.3. Interpolação a partir de Modelos de Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60e8b08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolacao_por_ml(df: pd.DataFrame,\n",
    "                        col_de_treino: list[str],\n",
    "                        var_de_predicao: str,\n",
    "                        var_de_pontos: str,\n",
    "                        n_de_teste: int):\n",
    "\n",
    "    # Obtendo pontos únicos\n",
    "    pontos_unicos = df[var_de_pontos].unique()\n",
    "\n",
    "    # Definição dos modelos\n",
    "    models = [\n",
    "\n",
    "        (\"ExtraTrees\", ExtraTreesRegressor(\n",
    "            n_estimators=15,\n",
    "            max_depth=20,\n",
    "            max_features=2,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            random_state=7\n",
    "        )),\n",
    "\n",
    "        (\"RandomForest\", RandomForestRegressor(\n",
    "            n_estimators=15,\n",
    "            max_depth=20,\n",
    "            max_features=2,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            random_state=7\n",
    "        )),\n",
    "\n",
    "        (\"KNeighbors\", KNeighborsRegressor(\n",
    "            n_neighbors=7,\n",
    "            weights='distance',\n",
    "            algorithm='auto'\n",
    "        )),\n",
    "\n",
    "        (\"GradientBoosting\", GradientBoostingRegressor(\n",
    "            n_estimators=20,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=20,\n",
    "            subsample=0.8,\n",
    "            random_state=7\n",
    "        )),\n",
    "\n",
    "    ]\n",
    "\n",
    "    # Lista para armazenar dados\n",
    "    datas = []\n",
    "\n",
    "    # Defininção de lista de métricas\n",
    "    metrics = []\n",
    "    for i in range(len(models)):\n",
    "        metrics.append([[], [], []])\n",
    "\n",
    "    for i in range(n_de_teste):\n",
    "\n",
    "        # Embaralha os pontos únicos\n",
    "        np.random.shuffle(pontos_unicos)\n",
    "\n",
    "        # Dividindo em 70% treino e 30% teste\n",
    "        split_idx = int(len(pontos_unicos) * 0.8)\n",
    "        pontos_treino = set(pontos_unicos[:split_idx])\n",
    "        pontos_teste = set(pontos_unicos[split_idx:])\n",
    "\n",
    "        # Criando DataFrames de treino e teste\n",
    "        df_treino = df[df[var_de_pontos].isin(pontos_treino)].copy()\n",
    "        df_teste = df[df[var_de_pontos].isin(pontos_teste)].copy()\n",
    "\n",
    "        # Definindo features (X) e variável alvo (y)\n",
    "        X_train = df_treino[col_de_treino]\n",
    "        y_train = df_treino[var_de_predicao]\n",
    "\n",
    "        X_test = df_teste[col_de_treino]\n",
    "        y_test = df_teste[var_de_predicao]\n",
    "\n",
    "        # Treinar e avaliar cada modelo\n",
    "        for j in range(len(models)):\n",
    "            models[j][1].fit(X_train, y_train)                                 # Treinamento\n",
    "            y_pred = models[j][1].predict(X_test)                              # Previsão\n",
    "            metrics[j][0].append(np.corrcoef(y_test, y_pred)[0, 1])            # r\n",
    "            metrics[j][1].append(np.sqrt(mean_squared_error(y_test, y_pred)))  # RMSE\n",
    "            metrics[j][2].append(np.sqrt(mean_absolute_error(y_test, y_pred))) # MAE\n",
    "\n",
    "    # Verificando melhor Modelo a partir de r\n",
    "    best_model = (0, '', '')\n",
    "\n",
    "    for i in range(len(metrics)):\n",
    "\n",
    "        r, rmse, mae = np.mean(metrics[i][0]), np.mean(metrics[i][1]), np.mean(metrics[i][2])\n",
    "\n",
    "        if best_model[0] < r:\n",
    "            best_model = r, models[i][0], models[i][1]\n",
    "\n",
    "        # print(f'Modelo: {models[i][0][:3]} \\t r: {r:.4f} \\t RMSE: {rmse:.4f} \\t MAE: {mae:.4f}')\n",
    "\n",
    "        datas.append((\n",
    "            f'{models[i][0][:3]}',\n",
    "            f'{r:.4f}',\n",
    "            f'{rmse:.4f}',\n",
    "            f'{mae:.4f}'\n",
    "        ))\n",
    "\n",
    "    # print(f'\\nO melhor modelo de ML para a base de dados é: {best_model[1]}.')\n",
    "\n",
    "    return best_model[2], datas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96757be9",
   "metadata": {},
   "source": [
    "### 3.1.4. Interpolação a partir de Modelo de Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b87debf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolacao_por_cnn(df: pd.DataFrame,\n",
    "                         col_de_treino: list[str],\n",
    "                         var_de_predicao: str,\n",
    "                         var_de_pontos: str,\n",
    "                         n_de_teste: int):\n",
    "\n",
    "    # Pontos únicos\n",
    "    pontos_unicos = df[var_de_pontos].unique()\n",
    "\n",
    "    # Listas para métricas\n",
    "    r_list = []\n",
    "    rmse_list = []\n",
    "    mae_list = []\n",
    "\n",
    "    # Lista para armazenar dados\n",
    "    datas = []\n",
    "\n",
    "    for i in range(n_de_teste):\n",
    "\n",
    "        np.random.shuffle(pontos_unicos)\n",
    "        split_idx = int(len(pontos_unicos) * 0.8)\n",
    "\n",
    "        pontos_treino = set(pontos_unicos[:split_idx])\n",
    "        pontos_teste = set(pontos_unicos[split_idx:])\n",
    "\n",
    "        df_treino = df[df[var_de_pontos].isin(pontos_treino)].copy()\n",
    "        df_teste = df[df[var_de_pontos].isin(pontos_teste)].copy()\n",
    "\n",
    "        # Definindo X e y\n",
    "        X_train = df_treino[col_de_treino].values\n",
    "        y_train = df_treino[var_de_predicao].values\n",
    "\n",
    "        X_test = df_teste[col_de_treino].values\n",
    "        y_test = df_teste[var_de_predicao].values\n",
    "\n",
    "        # Redimensionar para 3D: (samples, timesteps=1, features)\n",
    "        X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "        X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "        # Limpar sessão anterior (importante em loops com Keras)\n",
    "        clear_session()\n",
    "\n",
    "        # Criando modelo CNN\n",
    "        cnn = Sequential([\n",
    "            Input(shape=(1, X_train.shape[2])),\n",
    "            Conv1D(64, kernel_size=3, padding='same', activation='relu'),\n",
    "            MaxPooling1D(1),\n",
    "            Flatten(),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "\n",
    "        cnn.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mse'])\n",
    "\n",
    "        # Treinamento\n",
    "        cnn.fit(X_train, y_train, epochs=10, validation_split=0.2, verbose=0)\n",
    "\n",
    "        # Previsão e avaliação\n",
    "        y_pred = cnn.predict(X_test).flatten()\n",
    "\n",
    "        r_list.append(np.corrcoef(y_test, y_pred)[0, 1])\n",
    "        rmse_list.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "        mae_list.append(mean_absolute_error(y_test, y_pred))\n",
    "\n",
    "    # Resultados finais\n",
    "    # print(f'Modelo: CNN \\t r: {np.mean(r_list):.4f} \\t RMSE: {np.mean(rmse_list):.4f} \\t MAE: {np.mean(mae_list):.4f}')\n",
    "\n",
    "    datas.append((\n",
    "        'CNN',\n",
    "        f'{np.mean(r_list):.4f}',\n",
    "        f'{np.mean(rmse_list):.4f}',\n",
    "        f'{np.mean(mae_list):.4f}'\n",
    "    ))\n",
    "\n",
    "    return cnn, datas\n",
    "\n",
    "def interpolacao_por_mlp(df: pd.DataFrame,\n",
    "                         col_de_treino: list[str],\n",
    "                         var_de_predicao: str,\n",
    "                         var_de_pontos: str,\n",
    "                         n_de_teste: int):\n",
    "\n",
    "    # Pontos únicos\n",
    "    pontos_unicos = df[var_de_pontos].unique()\n",
    "\n",
    "    # Listas para métricas\n",
    "    r_list = []\n",
    "    rmse_list = []\n",
    "    mae_list = []\n",
    "\n",
    "    # Lista para armazenar dados\n",
    "    datas = []\n",
    "\n",
    "    for i in range(n_de_teste):\n",
    "\n",
    "        np.random.shuffle(pontos_unicos)\n",
    "        split_idx = int(len(pontos_unicos) * 0.8)\n",
    "\n",
    "        pontos_treino = set(pontos_unicos[:split_idx])\n",
    "        pontos_teste = set(pontos_unicos[split_idx:])\n",
    "\n",
    "        df_treino = df[df[var_de_pontos].isin(pontos_treino)].copy()\n",
    "        df_teste = df[df[var_de_pontos].isin(pontos_teste)].copy()\n",
    "\n",
    "        # Definindo X e y\n",
    "        X_train = df_treino[col_de_treino].values\n",
    "        y_train = df_treino[var_de_predicao].values\n",
    "\n",
    "        X_test = df_teste[col_de_treino].values\n",
    "        y_test = df_teste[var_de_predicao].values\n",
    "\n",
    "        # Limpar sessão anterior (importante em loops com Keras)\n",
    "        clear_session()\n",
    "\n",
    "        # Criando modelo MLP\n",
    "        mlp = Sequential([\n",
    "            Input(shape=(X_train.shape[1],)),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "\n",
    "        mlp.compile(optimizer=Adam(learning_rate=0.001), loss='mae', metrics=['mse', 'mae'])\n",
    "\n",
    "        # Treinamento\n",
    "        mlp.fit(X_train, y_train, epochs=10, validation_split=0.2, verbose=0)\n",
    "\n",
    "        # Previsão e avaliação\n",
    "        y_pred = mlp.predict(X_test).flatten()\n",
    "\n",
    "        r_list.append(np.corrcoef(y_test, y_pred)[0, 1])\n",
    "        rmse_list.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "        mae_list.append(mean_absolute_error(y_test, y_pred))\n",
    "\n",
    "    # Resultados finais\n",
    "    # print(f'Modelo: MLP \\t r: {np.mean(r_list):.4f} \\t RMSE: {np.mean(rmse_list):.4f} \\t MAE: {np.mean(mae_list):.4f}')\n",
    "\n",
    "    datas.append((\n",
    "        'MLP',\n",
    "        f'{np.mean(r_list):.4f}',\n",
    "        f'{np.mean(rmse_list):.4f}',\n",
    "        f'{np.mean(mae_list):.4f}'\n",
    "    ))\n",
    "\n",
    "    return mlp, datas\n",
    "\n",
    "def interpolacao_por_lstm(df: pd.DataFrame,\n",
    "                          col_de_treino: list[str],\n",
    "                          var_de_predicao: str,\n",
    "                          var_de_pontos: str,\n",
    "                          n_de_teste: int):\n",
    "\n",
    "    # Pontos únicos\n",
    "    pontos_unicos = df[var_de_pontos].unique()\n",
    "\n",
    "    # Listas para métricas\n",
    "    r_list = []\n",
    "    rmse_list = []\n",
    "    mae_list = []\n",
    "\n",
    "    # Lista para armazenar dados\n",
    "    datas = []\n",
    "\n",
    "    for i in range(n_de_teste):\n",
    "\n",
    "        np.random.shuffle(pontos_unicos)\n",
    "        split_idx = int(len(pontos_unicos) * 0.8)\n",
    "\n",
    "        pontos_treino = set(pontos_unicos[:split_idx])\n",
    "        pontos_teste = set(pontos_unicos[split_idx:])\n",
    "\n",
    "        df_treino = df[df[var_de_pontos].isin(pontos_treino)].copy()\n",
    "        df_teste = df[df[var_de_pontos].isin(pontos_teste)].copy()\n",
    "\n",
    "        # Definindo X e y\n",
    "        X_train = df_treino[col_de_treino].values\n",
    "        y_train = df_treino[var_de_predicao].values\n",
    "\n",
    "        X_test = df_teste[col_de_treino].values\n",
    "        y_test = df_teste[var_de_predicao].values\n",
    "\n",
    "        # Redimensionar para 3D: (samples, timesteps=1, features)\n",
    "        X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "        X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "        # Limpar sessão anterior (importante em loops com Keras)\n",
    "        clear_session()\n",
    "\n",
    "        # Modelo LSTM\n",
    "        lstm = Sequential([\n",
    "            Input(shape=(1, X_train.shape[2])),\n",
    "            LSTM(72, return_sequences=True),\n",
    "            LSTM(48, return_sequences=False),\n",
    "            Flatten(),\n",
    "            Dropout(0.2),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "\n",
    "        lstm.compile(optimizer=Adam(learning_rate=0.001), loss='mae', metrics=['mse', 'mae'])\n",
    "\n",
    "        # Treinamento\n",
    "        lstm.fit(X_train, y_train, epochs=10, validation_split=0.2, verbose=0)\n",
    "\n",
    "        # Previsão e avaliação\n",
    "        y_pred = lstm.predict(X_test).flatten()\n",
    "\n",
    "        r_list.append(np.corrcoef(y_test, y_pred)[0, 1])\n",
    "        rmse_list.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "        mae_list.append(mean_absolute_error(y_test, y_pred))\n",
    "\n",
    "    # Resultados finais\n",
    "    # print(f'Modelo: LSTM \\t r: {np.mean(r_list):.4f} \\t RMSE: {np.mean(rmse_list):.4f} \\t MAE: {np.mean(mae_list):.4f}')\n",
    "\n",
    "    datas.append((\n",
    "        'LSTM',\n",
    "        f'{np.mean(r_list):.4f}',\n",
    "        f'{np.mean(rmse_list):.4f}',\n",
    "        f'{np.mean(mae_list):.4f}'\n",
    "    ))\n",
    "\n",
    "    return lstm, datas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed6cfe3",
   "metadata": {},
   "source": [
    "### 3.1.5. Preenchimento para Serie Temporal Completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "423c4faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_temporal_series(df: pd.DataFrame,\n",
    "                             lat_col: str,\n",
    "                             lon_col: str,\n",
    "                             anos: list) -> pd.DataFrame:\n",
    "    '''\n",
    "    Preenchimento de datas faltantes em séries temporais\n",
    "    '''\n",
    "\n",
    "    # Obtendo pontos de lat lon únicos\n",
    "    df_pontos_unicos = df[[lat_col, lon_col]].drop_duplicates().reset_index().drop(columns=[\"index\"])\n",
    "\n",
    "    # Obtendo meses e anos da base de dados\n",
    "    df_anos_meses = pd.DataFrame([(ano, mes) for ano in anos for mes in range(1, 13)], columns=[\"ano\", \"mes\"])\n",
    "\n",
    "    # Fazendo produto cartesiano entre pontos e anos/meses\n",
    "    df_pontos_unicos['key'] = 1\n",
    "    df_anos_meses['key'] = 1\n",
    "\n",
    "    # Unindo pontos e anos/meses\n",
    "    df_expandido = pd.merge(df_pontos_unicos, df_anos_meses, on='key').drop(columns='key')\n",
    "\n",
    "    # Cria um DataFrame vazio para armazenar os dados expandidos\n",
    "    return df_expandido"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdc8762",
   "metadata": {},
   "source": [
    "### 3.1.6. Função para preencher nulos por Interpolação por IDW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2111265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def porcentagem_em_barra(valor_atual: int,\n",
    "                         valor_total: int) -> str:\n",
    "    \"\"\"\n",
    "    Gerador de barra de porcentagem a partir de valor atual e total.\n",
    "    \"\"\"\n",
    "\n",
    "    porcentagem = 100 * (valor_atual / valor_total)\n",
    "\n",
    "    completo   = '━' * (int(porcentagem))\n",
    "    incompleto = '╺' * (100 - int(porcentagem))\n",
    "\n",
    "    situacao = f'[{completo}{incompleto}] {porcentagem:.2f}% ({valor_atual} de {valor_total})'\n",
    "\n",
    "    return situacao\n",
    "\n",
    "def idw_interpolation(target_point, neighbors, values, const=2):\n",
    "    \"\"\"\n",
    "    Interpolação IDW: valores dos vizinhos ponderados pela distância\n",
    "    \"\"\"\n",
    "\n",
    "    weights = []\n",
    "    for pt in neighbors:\n",
    "        dist = geodesic(target_point, pt).meters\n",
    "        if dist == 0:\n",
    "            continue  # Evita usar o próprio ponto\n",
    "        weights.append(1 / (dist ** const))\n",
    "    weights = np.array(weights)\n",
    "    values = np.array(values)\n",
    "    return np.sum(weights * values) / np.sum(weights)\n",
    "\n",
    "def preencher_precipitacao_idw(df: pd.DataFrame,\n",
    "                               var_de_interpolacao: str,\n",
    "                               var_de_latitude: str,\n",
    "                               var_de_longitude: str,\n",
    "                               var_de_anos: str,\n",
    "                               var_de_meses: str,\n",
    "                               n_vizinhos: int = 10,\n",
    "                               const: int = 2) -> pd.DataFrame:\n",
    "\n",
    "    df_resultado = df.copy()\n",
    "\n",
    "    # Iterar sobre os índices com valores faltantes\n",
    "    missing_indices = df_resultado[df_resultado[var_de_interpolacao].isna()].index\n",
    "\n",
    "    valor_atual, valor_total = 1, len(missing_indices)\n",
    "\n",
    "    for idx in missing_indices:\n",
    "\n",
    "        row = df_resultado.loc[idx]\n",
    "        ano, mes, lat, lon = row[var_de_anos], row[var_de_meses], row[var_de_latitude], row[var_de_longitude]\n",
    "        target_point = (lat, lon)\n",
    "\n",
    "        # Filtrar pontos com mesma data e precipitação conhecida\n",
    "        filtro = (df_resultado[var_de_anos] == ano) & (df_resultado[var_de_meses] == mes) & df_resultado[var_de_interpolacao].notna()\n",
    "        candidatos = df_resultado[filtro].copy()\n",
    "\n",
    "        # Calcular distâncias\n",
    "        candidatos['distancia'] = candidatos.apply(lambda r: geodesic(target_point, (r[var_de_latitude], r[var_de_longitude])).meters, axis=1)\n",
    "\n",
    "        # Selecionar os pontos mais próximos\n",
    "        vizinhos = candidatos.nsmallest(n_vizinhos, 'distancia')\n",
    "\n",
    "        if not vizinhos.empty:\n",
    "            viz_points = list(zip(vizinhos[var_de_latitude], vizinhos[var_de_longitude]))\n",
    "            viz_values = vizinhos[var_de_interpolacao].tolist()\n",
    "\n",
    "            # Aplicar IDW\n",
    "            interpolado = idw_interpolation(target_point, viz_points, viz_values, const)\n",
    "            df_resultado.at[idx, var_de_interpolacao] = interpolado\n",
    "\n",
    "        if valor_atual % 10 == 0 or valor_atual == valor_total:\n",
    "            clear()\n",
    "            print(porcentagem_em_barra(valor_atual, valor_total))\n",
    "\n",
    "        valor_atual += 1\n",
    "\n",
    "    return df_resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bcb420",
   "metadata": {},
   "source": [
    "## 3.2. Coluna IDW para dados da AESA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f513bf8",
   "metadata": {},
   "source": [
    "### 3.2.1. Configurando Bases de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49e267a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Informações Locais:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 87120 entries, 0 to 87119\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Unnamed: 0  87120 non-null  int64  \n",
      " 1   lat         87120 non-null  float64\n",
      " 2   lon         87120 non-null  float64\n",
      " 3   ano         87120 non-null  int64  \n",
      " 4   mes         87120 non-null  int64  \n",
      " 5   IDW         87120 non-null  float64\n",
      " 6   pr_local    87120 non-null  float64\n",
      "dtypes: float64(4), int64(3)\n",
      "memory usage: 4.7 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Base de dados local de acumulados\n",
    "df_aesa = pd.read_csv(\n",
    "    f\"../datas/interim/2.3.1_aesa_database_create/aesa_1994-2023_mon_{database_type}.csv\"\n",
    ")\n",
    "\n",
    "# Caso não tenha a coluna IDW nas \"colunas_de_interesse\", calcula-se o IDW\n",
    "if idw_method == True:\n",
    "\n",
    "    if idw_generate == True:  # 50m 54.6s\n",
    "\n",
    "        # Local\n",
    "        df_aesa = calcular_precipitacao_idw(df_aesa, \"pr_local\", \"lat\", \"lon\", \"ano\", \"mes\")\n",
    "        df_aesa[['lat', 'lon',\n",
    "                 'ano', 'mes',\n",
    "                 'IDW', 'pr_local'\n",
    "        ]].to_csv(f'../datas/interim/3.2.1_aesa_with_idw/aesa_1994-2023_mon_{database_type}_idw.csv')\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Local\n",
    "        df_aesa = pd.read_csv(f'../datas/interim/3.2.1_aesa_with_idw/aesa_1994-2023_mon_{database_type}_idw.csv')\n",
    "\n",
    "# Visualizando Bases de Dados Locais\n",
    "print('- Informações Locais:')\n",
    "print(df_aesa.info())\n",
    "# grafico_de_pontos(df_aesa, \"lat\", \"lon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862258f5",
   "metadata": {},
   "source": [
    "## 3.3. Base de Dados Local para Predição\n",
    "\n",
    "```python\n",
    "O processo consiste em criar uma base de dados com os mesmos \n",
    "pontos da base de dados do GCM, para que assim se possa realizar \n",
    "a interpolação da precipitação local para esses pontos.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9afb457",
   "metadata": {},
   "source": [
    "### 3.3.1. Configurando Base de Dados Local para Predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "949ad7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18000 entries, 0 to 17999\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   lat     18000 non-null  float64\n",
      " 1   lon     18000 non-null  float64\n",
      " 2   ano     18000 non-null  int64  \n",
      " 3   mes     18000 non-null  int64  \n",
      "dtypes: float64(2), int64(2)\n",
      "memory usage: 562.6 KB\n"
     ]
    }
   ],
   "source": [
    "# Geração ou abertura de base de dados gerada\n",
    "if databases_generate == True:\n",
    "\n",
    "    # Definindo base de dados de GCM\n",
    "    df_cnrm_cm6_1hr = pd.read_csv(\n",
    "        f\"../datas/interim/1.3.2_cmip6_database_create/pr_day_CNRM-CM6-1-HR_ssp585_r1i1p1f2_gr_19940101-21001231_{database_type}.csv\"\n",
    "    )\n",
    "\n",
    "    # Definindo base de dados\n",
    "    df_aesa_to_cnrm_cm6_1hr = complete_temporal_series(df_cnrm_cm6_1hr, 'lat', 'lon', [i for i in range(1994, 2024)])\n",
    "\n",
    "    # Cria uma coluna \"pnt\" que combina latitude e longitude como string separada por ponto e vírgula\n",
    "    # df_aesa_to_cnrm_cm6_1hr['pnt'] = df_aesa_to_cnrm_cm6_1hr[\"lat\"].astype(str) + \";\" + df_aesa_to_cnrm_cm6_1hr[\"lon\"].astype(str)\n",
    "\n",
    "    # Exportando base de dados gerada\n",
    "    df_aesa_to_cnrm_cm6_1hr.to_csv(f'../datas/interim/3.3.1_aesa_in_cmip6_points/aesa_to_cnrm_cm6_1hr_{database_type}.csv')\n",
    "\n",
    "else:\n",
    "\n",
    "    # Abrindo base de dados\n",
    "    df_aesa_to_cnrm_cm6_1hr = pd.read_csv(f'../datas/interim/3.3.1_aesa_in_cmip6_points/aesa_to_cnrm_cm6_1hr_{database_type}.csv')\n",
    "\n",
    "# Informaões da base de dados gerada\n",
    "df_aesa_to_cnrm_cm6_1hr.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a23e461",
   "metadata": {},
   "source": [
    "### 3.3.2. Configurando Base de Dados Local para Predição com IDW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04979478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18000 entries, 0 to 17999\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Unnamed: 0  18000 non-null  int64  \n",
      " 1   lat         18000 non-null  float64\n",
      " 2   lon         18000 non-null  float64\n",
      " 3   ano         18000 non-null  int64  \n",
      " 4   mes         18000 non-null  int64  \n",
      " 5   IDW         18000 non-null  float64\n",
      "dtypes: float64(3), int64(3)\n",
      "memory usage: 843.9 KB\n"
     ]
    }
   ],
   "source": [
    "# Caso não tenha a coluna IDW nas \"colunas_de_interesse\", calcula-se o IDW\n",
    "if idw_method == True:\n",
    "\n",
    "    if idw_generate == True:  # 25m 35.1s\n",
    "\n",
    "        df_temp = pd.merge(df_aesa_to_cnrm_cm6_1hr, df_aesa[['lat', 'lon', 'ano', 'mes', 'pr_local']],\n",
    "                            on=['lat', 'lon', 'ano', 'mes'],\n",
    "                            how='outer')\n",
    "\n",
    "        indices_com_nulos = df_temp[df_temp.isnull().any(axis=1)].index\n",
    "\n",
    "        df_temp = preencher_precipitacao_idw(df_temp, \"pr_local\", \"lat\", \"lon\", \"ano\", \"mes\")\n",
    "\n",
    "        df_temp = df_temp.loc[indices_com_nulos]\n",
    "\n",
    "        df_temp = df_temp.rename(columns={'pr_local': 'IDW'})\n",
    "\n",
    "        df_temp.to_csv(f'../datas/interim/3.3.2_aesa_in_cmip6_points_with_idw/aesa_to_cnrm_cm6_1hr_{database_type}_idw.csv')\n",
    "\n",
    "        df_aesa_to_cnrm_cm6_1hr = df_temp.copy()\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Abrindo base de dados\n",
    "        df_aesa_to_cnrm_cm6_1hr = pd.read_csv(f'../datas/interim/3.3.2_aesa_in_cmip6_points_with_idw/aesa_to_cnrm_cm6_1hr_{database_type}_idw.csv')\n",
    "\n",
    "# Informaões da base de dados gerada\n",
    "df_aesa_to_cnrm_cm6_1hr.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc13927",
   "metadata": {},
   "source": [
    "#### 3.3.3. Configurando Modelo Preditivo de Interpolação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c07505b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━] 100.00% (15381 de 15381)\n",
      "\n",
      "Verificação de Modelo Base:\n",
      "\n",
      "Modelo: IDW \t r: 0.9270 \t RMSE: 30.8732 \t MAE: 4.3063\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Definindo pontos únicos\n",
    "df_aesa['pnt'] = df_aesa[\"lat\"].astype(str) + \";\" + df_aesa[\"lon\"].astype(str)\n",
    "\n",
    "# Definindo Modelo Base\n",
    "df_baseline_idw = df_aesa[df_aesa['pr_local'] != df_aesa['IDW']][['lat', 'lon', 'ano', 'mes', 'pr_local', 'pnt']]\n",
    "\n",
    "# Obtendo pontos únicos\n",
    "pontos_unicos = df_aesa['pnt'].unique()\n",
    "\n",
    "# Dividindo em 70% treino e 30% teste\n",
    "split_idx = int(len(pontos_unicos) * 0.8)\n",
    "pontos_treino = set(pontos_unicos[:split_idx])\n",
    "pontos_teste = set(pontos_unicos[split_idx:])\n",
    "\n",
    "# Criando as bases de treino e teste\n",
    "df_baseline_idw_treino = df_baseline_idw.copy()\n",
    "df_baseline_idw_treino.loc[df_baseline_idw_treino['pnt'].isin(pontos_teste), 'pr_local'] = np.nan\n",
    "df_baseline_idw_teste = df_baseline_idw[df_baseline_idw['pnt'].isin(pontos_teste)].copy()\n",
    "\n",
    "# Calculando IDW\n",
    "df_baseline_idw_treino = preencher_precipitacao_idw(df_baseline_idw_treino, 'pr_local', 'lat', 'lon', 'ano', 'mes', 10)  # 7m 11.1s\n",
    "df_baseline_idw_treino = df_baseline_idw_treino[df_baseline_idw_treino['pnt'].isin(pontos_teste)].copy()\n",
    "\n",
    "# Calculando acurácias\n",
    "r = np.corrcoef(df_baseline_idw_treino['pr_local'], df_baseline_idw_teste['pr_local'])[0, 1]\n",
    "rmse = np.sqrt(mean_squared_error(df_baseline_idw_treino['pr_local'], df_baseline_idw_teste['pr_local']))\n",
    "mae = np.sqrt(mean_absolute_error(df_baseline_idw_treino['pr_local'], df_baseline_idw_teste['pr_local']))\n",
    "\n",
    "print(f'\\nVerificação de Modelo Base:\\n\\nModelo: IDW \\t r: {r:.4f} \\t RMSE: {rmse:.4f} \\t MAE: {mae:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6a464f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificação de Modelos:\n",
      "\n",
      "Modelo: Ext \t r: 0.9429 \t RMSE: 28.9271 \t MAE: 4.1104\n",
      "Modelo: Ran \t r: 0.9415 \t RMSE: 29.3022 \t MAE: 4.1378\n",
      "Modelo: KNe \t r: 0.9378 \t RMSE: 30.1935 \t MAE: 4.2120\n",
      "Modelo: Gra \t r: 0.9379 \t RMSE: 41.1795 \t MAE: 5.3948\n",
      "Modelo: CNN \t r: 0.9484 \t RMSE: 34.7361 \t MAE: 20.3816\n",
      "Modelo: MLP \t r: 0.9436 \t RMSE: 47.6710 \t MAE: 27.5590\n",
      "Modelo: LSTM \t r: 0.9275 \t RMSE: 34.4790 \t MAE: 20.7947\n"
     ]
    }
   ],
   "source": [
    "# Definindo colunas para treino\n",
    "if idw_method == True:\n",
    "    columns_X = [\"lat\", \"lon\", \"ano\", \"mes\", \"IDW\"]\n",
    "else:\n",
    "    columns_X = [\"lat\", \"lon\", \"ano\", \"mes\"]\n",
    "\n",
    "# Escolhendo melhor modelo preditivo de ML\n",
    "model_ml, datas_ml = interpolacao_por_ml(df_aesa, columns_X, \"pr_local\", \"pnt\", 5)\n",
    "\n",
    "# Escolhendo melhor modelo preditivo de DL\n",
    "model_cnn, datas_cnn = interpolacao_por_cnn(df_aesa, columns_X, \"pr_local\", \"pnt\", 5)     # CNN\n",
    "model_mlp, datas_mlp = interpolacao_por_mlp(df_aesa, columns_X, \"pr_local\", \"pnt\", 5)     # MLP\n",
    "model_lstm, datas_lstm = interpolacao_por_lstm(df_aesa, columns_X, \"pr_local\", \"pnt\", 5)  # LSTM\n",
    "\n",
    "# Agrupando dados\n",
    "datas = datas_ml + datas_cnn + datas_mlp + datas_lstm\n",
    "\n",
    "# Exibindo resultados\n",
    "clear()\n",
    "print('Verificação de Modelos:\\n')\n",
    "for model, r, rmse, mae in datas:\n",
    "    print(f'Modelo: {model} \\t r: {r} \\t RMSE: {rmse} \\t MAE: {mae}')\n",
    "\n",
    "# O melhor modelo obtido é o ExtraTrees, dado a combinação r, RMSE e MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46974b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhor combinação de hiperparâmetros:\n",
      "\n",
      "{'max_depth': 100, 'max_features': 2, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "\n",
      "r: 0.9437 \t RMSE: 27.1211 \t MAE: 3.9758\n"
     ]
    }
   ],
   "source": [
    "# Definindo base de dados para busca de hiperparâmetros do melhor modelo\n",
    "df_aesa_best_model = df_aesa.copy()\n",
    "\n",
    "# Definindo variáveis necessárias\n",
    "var_de_pontos = \"pnt\"\n",
    "n_de_teste = 5\n",
    "col_de_treino = columns_X\n",
    "var_de_predicao = \"pr_local\"\n",
    "\n",
    "# Obtendo pontos únicos\n",
    "pontos_unicos = df_aesa_best_model[var_de_pontos].unique()\n",
    "\n",
    "# Parâmetros para grid search\n",
    "param_grid = {\n",
    "    \"n_estimators\": [15, 50, 100],\n",
    "    \"max_depth\": [15, 50, 100],\n",
    "    \"max_features\": [2, 3],\n",
    "    \"min_samples_split\": [2, 4],\n",
    "    \"min_samples_leaf\": [1, 2]\n",
    "}\n",
    "\n",
    "# Lista para armazenar resultados\n",
    "resultados = []\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "\n",
    "    # Defininção de lista de métricas\n",
    "    metrics = [[], [], []]\n",
    "\n",
    "    for i in range(n_de_teste):\n",
    "\n",
    "        # Embaralha os pontos únicos\n",
    "        np.random.shuffle(pontos_unicos)\n",
    "\n",
    "        # Dividindo em 70% treino e 30% teste\n",
    "        split_idx = int(len(pontos_unicos) * 0.8)\n",
    "        pontos_treino = set(pontos_unicos[:split_idx])\n",
    "        pontos_teste = set(pontos_unicos[split_idx:])\n",
    "\n",
    "        # Criando DataFrames de treino e teste\n",
    "        df_treino = df_aesa_best_model[df_aesa_best_model[var_de_pontos].isin(pontos_treino)].copy()\n",
    "        df_teste = df_aesa_best_model[df_aesa_best_model[var_de_pontos].isin(pontos_teste)].copy()\n",
    "\n",
    "        # Definindo features (X) e variável alvo (y)\n",
    "        X_train = df_treino[col_de_treino]\n",
    "        y_train = df_treino[var_de_predicao]\n",
    "\n",
    "        X_test = df_teste[col_de_treino]\n",
    "        y_test = df_teste[var_de_predicao]\n",
    "\n",
    "        model = ExtraTreesRegressor(**params, random_state=7)           # Modelo\n",
    "        model.fit(X_train, y_train)                                     # Treinamento\n",
    "        y_pred = model.predict(X_test)                                  # Previsão\n",
    "\n",
    "        metrics[0].append(np.corrcoef(y_test, y_pred)[0, 1])            # r\n",
    "        metrics[1].append(np.sqrt(mean_squared_error(y_test, y_pred)))  # RMSE\n",
    "        metrics[2].append(np.sqrt(mean_absolute_error(y_test, y_pred))) # MAE\n",
    "\n",
    "    # Média das métricas\n",
    "    avg_r = np.mean(metrics[0])\n",
    "    avg_rmse = np.mean(metrics[1])\n",
    "    avg_mae = np.mean(metrics[2])\n",
    "\n",
    "    resultados.append({\n",
    "        'model': model,\n",
    "        'params': params,\n",
    "        'r': avg_r,\n",
    "        'RMSE': avg_rmse,\n",
    "        'MAE': avg_mae\n",
    "    })\n",
    "\n",
    "# Ordenando pelos melhores modelos (menor RMSE)\n",
    "resultados_ordenados = sorted(resultados, key=lambda x: x['RMSE'], reverse=False)\n",
    "\n",
    "# Exibindo o melhor\n",
    "melhor_modelo = resultados_ordenados[0]\n",
    "\n",
    "print(\"Melhor combinação de hiperparâmetros:\")\n",
    "print(f\"\\n{melhor_modelo['params']}\")\n",
    "print(f\"\\nr: {melhor_modelo['r']:.4f} \\t RMSE: {melhor_modelo['RMSE']:.4f} \\t MAE: {melhor_modelo['MAE']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb61b41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo features (X) e variável alvo (y)\n",
    "X = df_aesa[columns_X].copy()\n",
    "y = df_aesa[\"pr_local\"].copy()\n",
    "\n",
    "# Melhores hiperparâmetros encontrados\n",
    "best_params = {\n",
    "    'max_depth': 15,\n",
    "    'max_features': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_samples_split': 4,\n",
    "    'n_estimators': 50\n",
    "}\n",
    "\n",
    "# Criando e treinando o modelo com todos os dados\n",
    "model = ExtraTreesRegressor(**best_params, random_state=7)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Treinamento\n",
    "model.fit(X, y)\n",
    "\n",
    "# Salvando o modelo\n",
    "if databases_generate:\n",
    "    joblib.dump(model_ml, f'../models/local_data_spatialization.joblib', compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a14df54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18000 entries, 0 to 17999\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   lat       18000 non-null  float64\n",
      " 1   lon       18000 non-null  float64\n",
      " 2   ano       18000 non-null  int64  \n",
      " 3   mes       18000 non-null  int64  \n",
      " 4   pr_local  18000 non-null  float64\n",
      "dtypes: float64(3), int64(2)\n",
      "memory usage: 703.2 KB\n"
     ]
    }
   ],
   "source": [
    "# Previsão dos valores de 'pr_local' com o modelo treinado\n",
    "pr_local = model.predict(df_aesa_to_cnrm_cm6_1hr[columns_X])\n",
    "\n",
    "# Colunas do Modelo\n",
    "columns = ['lat', 'lon', 'ano', 'mes', 'pr_local']\n",
    "\n",
    "# Adicionando a nova coluna 'pr_local' ao DataFrame\n",
    "df_aesa_to_cnrm_cm6_1hr['pr_local'] = pr_local\n",
    "\n",
    "# Salvando dados preditos\n",
    "df_aesa_to_cnrm_cm6_1hr[columns].to_csv(f'../datas/interim/3.3.3_aesa_interpolated_to_cmip6/aesa_to_cnrm_cm6_1hr_{database_type}_interpolated.csv')\n",
    "\n",
    "# Exibindo as primeiras linhas para verificar\n",
    "df_aesa_to_cnrm_cm6_1hr[columns].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1759c3c3",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>Código em desuso...</summary>\n",
    "\n",
    "## 3.3. Base de Dados GCM para Predição (Inutilizado)\n",
    "\n",
    "```shell\n",
    "O processo consiste em criar uma base de dados com os mesmos\n",
    "pontos da base de dados locais, para que assim se possa realizar\n",
    "a interpolação da precipitação do GCM para esses pontos.\n",
    "```\n",
    "\n",
    "### 3.3.1. Configurando Base de Dados do GCM para Predição\n",
    "\n",
    "```python\n",
    "# Geração ou abertura de base de dados gerada\n",
    "if databases_generate == True:\n",
    "\n",
    "    # Definindo base de dados\n",
    "    df_cnrm_cm6_1hr_to_aesa = complete_temporal_series(df_aesa, 'lat', 'lon', [i for i in range(1994, 2024)])\n",
    "\n",
    "    # Cria uma coluna \"pnt\" que combina latitude e longitude como string separada por ponto e vírgula\n",
    "    df_cnrm_cm6_1hr_to_aesa['pnt'] = df_cnrm_cm6_1hr_to_aesa[\"lat\"].astype(str) + \";\" + df_cnrm_cm6_1hr_to_aesa[\"lon\"].astype(str)\n",
    "\n",
    "    # Exportando base de dados gerada\n",
    "    df_cnrm_cm6_1hr_to_aesa.to_csv(f'cnrm_cm6_1hr_to_aesa_{database_type}.csv')\n",
    "\n",
    "else:\n",
    "\n",
    "    # Abrindo base de dados\n",
    "    df_cnrm_cm6_1hr_to_aesa = pd.read_csv(f'cnrm_cm6_1hr_to_aesa_{database_type}.csv')\n",
    "\n",
    "# Informaões da base de dados gerada\n",
    "df_cnrm_cm6_1hr_to_aesa.info()\n",
    "```\n",
    "\n",
    "#### 3.3.2. Configurando Base de Dados de GCM para Predição com IDW\n",
    "\n",
    "```python\n",
    "# Caso não tenha a coluna IDW nas \"colunas_de_interesse\", calcula-se o IDW\n",
    "if idw_method == True:\n",
    "\n",
    "    if idw_generate == True:  # 97m 54.1s\n",
    "\n",
    "        # Adicionando coluna IDW à base de dados\n",
    "\n",
    "        df_temp = pd.DataFrame()\n",
    "\n",
    "        for _, row in df_cnrm_cm6_1hr_to_aesa.iterrows():\n",
    "\n",
    "            row_temp = interpolacao_por_idw(pd.concat([df_cnrm_cm6_1hr, pd.DataFrame([row])], ignore_index=True), \"pr\", \"ano\", \"mes\", \"pnt\", row['pnt'], False)\n",
    "\n",
    "            df_temp = pd.concat([df_temp, row_temp[row_temp['pnt'] == row['pnt']]], ignore_index=True)\n",
    "\n",
    "            print(porcentagem_em_barra(_+1, len(df_cnrm_cm6_1hr_to_aesa)))\n",
    "\n",
    "        df_temp.to_csv(f'cnrm_cm6_1hr_to_aesa_{database_type}_idw.csv')\n",
    "\n",
    "        df_cnrm_cm6_1hr_to_aesa = df_temp.copy()\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Abrindo base de dados\n",
    "        df_cnrm_cm6_1hr_to_aesa = pd.read_csv(f'cnrm_cm6_1hr_to_aesa_{database_type}_idw.csv')\n",
    "\n",
    "# Informaões da base de dados gerada\n",
    "df_cnrm_cm6_1hr_to_aesa.info()\n",
    "```\n",
    "\n",
    "#### 3.3.3. Configurando Modelo Preditivo de Interpolação\n",
    "\n",
    "```python\n",
    "# Definindo colunas para treino\n",
    "if idw_method == True:\n",
    "    columns_X = [\"lat\", \"lon\", \"ano\", \"mes\", \"IDW\"]\n",
    "else:\n",
    "    columns_X = [\"lat\", \"lon\", \"ano\", \"mes\"]\n",
    "\n",
    "# Escolhendo melhor modelo preditivo\n",
    "model = interpolacao_por_ml(df_cnrm_cm6_1hr, columns_X, \"pr\", \"pnt\", 5)\n",
    "\n",
    "# Definindo features (X) e variável alvo (y)\n",
    "X = df_cnrm_cm6_1hr[columns_X].copy()\n",
    "y = df_cnrm_cm6_1hr[\"pr\"].copy()\n",
    "\n",
    "# Treinamento\n",
    "model.fit(X, y)\n",
    "```\n",
    "\n",
    "```python\n",
    "# Previsão dos valores de 'pr' com o modelo treinado\n",
    "pr = model.predict(df_cnrm_cm6_1hr_to_aesa[columns_X])\n",
    "\n",
    "# Colunas do Modelo\n",
    "columns = ['lat', 'lon', 'ano', 'mes', 'pr']\n",
    "\n",
    "# Adicionando a nova coluna 'pr' ao DataFrame\n",
    "df_cnrm_cm6_1hr_to_aesa['pr'] = pr\n",
    "\n",
    "# Salvando dados preditos\n",
    "df_cnrm_cm6_1hr_to_aesa[columns].to_csv(f'3-INTERPOLACAO/3.2/3.2.3/3.2.3.3/cnrm_cm6_1hr_to_aesa_{database_type}.csv')\n",
    "\n",
    "# Exibindo as primeiras linhas para verificar\n",
    "df_cnrm_cm6_1hr_to_aesa[columns].info()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
